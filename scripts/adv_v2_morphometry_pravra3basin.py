# -*- coding: utf-8 -*-
"""
=============================================================================
 ADV_V2_Morphometry_Pravra3Basin.py
=============================================================================
 Pravra River Basin Morphometric Analysis â€” 3 Pour-Point Version
 Based on: adv_v1_morphometry_pravarabasin.py

 KEY CHANGES from V1 (5-subbasin version):
   1. DATA_PATHS['subbasins']  â†’ Pravrabasin.shp
                                  (Contains 5 polygons as shipped in zip.
                                   Replace with pravra3.shp once available
                                   to get a true 3-subbasin run.)
   2. DATA_PATHS['pour_points'] â†’ Pourpoints_3.shp  (3 points âœ…)
   3. DATA_PATHS['streams']     â†’ SteamOrder.shp    (was broken path /content/streams.shp)
   4. N_SUBBASINS               â†’ Auto-detected from shapefile (no hard assertion crash)
   5. DEM resolution check      â†’ Warning only, not hard assert
   6. Basin ID detection        â†’ Smart extraction from Pravrabasin name columns
   7. ORDER_COL                 â†’ 'grid_code' (confirmed from SteamOrder.dbf)
   8. ChannelOverlandFlow_C     â†’ Fixed to ChannelMaintenance_C (correct column name)

 SHAPEFILE STATUS (verified from zip: Morphomtery_layers-Final.zip):
   âœ… Pravrabasin.shp   â€” Polygon,   5 records,  UTM Zone 43N, all sidecars OK
   âœ… Pourpoints_3.shp  â€” Point,     3 records,  UTM Zone 43N, all sidecars OK
   âœ… SteamOrder.shp    â€” Polyline,  3610 segs,  UTM Zone 43N, all sidecars OK
   âœ… Filled DEM.tif    â€” Raster,    UTM Zone 43N
   âœ… Flow Direction.tif â€” Raster,   UTM Zone 43N
   âœ… FlowAccumilation.tif â€” Raster, UTM Zone 43N

 Run in Google Colab with GPU disabled (CPU is sufficient).
=============================================================================
"""

"""
=============================================================================
SECTION 0 â€” ZIP EXTRACTION & FILE DISCOVERY
=============================================================================
Upload your zip file to Google Colab and run this section first.
It will extract all files and auto-detect the required layers.
=============================================================================
"""

import os
import zipfile
import glob

# â”€â”€ USER INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  UPDATED for 3 Pour-Point Pravra Basin run                             â”‚
# â”‚  Zip file:  "Morphomtery_layers-Final.zip"                             â”‚
# â”‚  Subbasins: Pravrabasin.shp  (5 polygons â€” use as-is, or replace with â”‚
# â”‚             a 3-polygon shapefile named pravra3.shp when available)     â”‚
# â”‚  Pour pts:  Pourpoints_3.shp  (3 points â€” confirmed in zip)           â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ZIP_PATH    = "/content/Morphomtery_layers-Final.zip"   # â† Updated zip name
EXTRACT_DIR = "/content/watershed_data/"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_zip(zip_path, extract_dir):
    """Extract zip file and list contents."""
    os.makedirs(extract_dir, exist_ok=True)
    if not os.path.exists(zip_path):
        raise FileNotFoundError(
            f"ZIP not found at {zip_path}. "
            "Please upload your zip file to Colab first."
        )
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_dir)
        names = z.namelist()
    print(f"âœ… Extracted {len(names)} files to {extract_dir}")
    return names


def discover_files(extract_dir):
    """
    Auto-detect required GIS layers from extracted directory.
    Returns a dict of file paths.
    """
    files = {}

    # Walk all subdirectories
    all_files = []
    for root, dirs, fnames in os.walk(extract_dir):
        for f in fnames:
            all_files.append(os.path.join(root, f))

    print("\nğŸ“‚ All extracted files:")
    for f in all_files:
        print(f"   {f}")

    # â”€â”€ RASTERS (.tif / .img / .asc) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rasters = [f for f in all_files if f.lower().endswith(('.tif', '.tiff', '.img', '.asc'))]

    # Keyword-based auto-detection (case-insensitive)
    for r in rasters:
        base = os.path.basename(r).lower()
        if any(k in base for k in ['dem', 'srtm', 'elevation', 'filled', 'fill']):
            files['dem'] = r
        elif any(k in base for k in ['flowdir', 'flow_dir', 'fdir', 'direction']):
            files['flow_dir'] = r
        elif any(k in base for k in ['flowacc', 'flow_acc', 'facc', 'accumulation']):
            files['flow_acc'] = r
        elif any(k in base for k in ['strahler', 'streamorder', 'stream_order', 'order']):
            files['stream_order_raster'] = r
        elif any(k in base for k in ['slope']):
            files['slope'] = r
        elif any(k in base for k in ['aspect']):
            files['aspect'] = r

    # â”€â”€ VECTORS (.shp) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    shapefiles = [f for f in all_files if f.lower().endswith('.shp')]

    for s in shapefiles:
        base = os.path.basename(s).lower()
        if any(k in base for k in ['subbasin', 'sub_basin', 'watershed', 'basin', 'catchment', 'pravra']):
            files['Subbasins'] = s
        elif any(k in base for k in ['stream', 'river', 'channel', 'network', 'drainage', 'steam']):
            if 'order' in base or 'steam' in base:
                files['stream_order_shp'] = s
                files['streams'] = s   # SteamOrder.shp doubles as streams
            else:
                files['streams'] = s
        elif any(k in base for k in ['pour', 'outlet', 'point']):
            files['pour_points'] = s
        elif any(k in base for k in ['order']):
            files['stream_order_shp'] = s

    # â”€â”€ FALLBACK: if stream_order_shp not found, use streams â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if 'stream_order_shp' not in files and 'streams' in files:
        files['stream_order_shp'] = files['streams']

    print("\nğŸ—ºï¸  Auto-detected layers:")
    for key, val in files.items():
        print(f"   {key:25s} â†’ {val}")

    missing = []
    required = ['dem', 'subbasins', 'streams', 'flow_dir', 'flow_acc']
    for req in required:
        if req not in files:
            missing.append(req)

    if missing:
        print(f"\nâš ï¸  Could not auto-detect: {missing}")
        print("   Please set paths manually in SECTION 2 â€” DATA PATHS.")
    else:
        print("\nâœ… All required layers detected.")

    return files


# â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    extract_zip(ZIP_PATH, EXTRACT_DIR)
    DETECTED_FILES = discover_files(EXTRACT_DIR)

    # Print for copy-paste into Section 2
    print("\n" + "="*60)
    print("ğŸ“‹ Copy these paths into SECTION 2 â€” DATA PATHS:")
    print("="*60)
    for k, v in DETECTED_FILES.items():
        print(f'  "{k}": r"{v}",')

"""
=============================================================================
SECTION 1 â€” ENVIRONMENT SETUP & LIBRARY IMPORTS
=============================================================================
Run in Google Colab. Installs missing packages and imports all libraries.
=============================================================================
"""

import subprocess, sys

def pip_install(*pkgs):
    """Silent pip install with error catching."""
    for pkg in pkgs:
        try:
            subprocess.check_call(
                [sys.executable, "-m", "pip", "install", pkg, "-q"],
                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
            print(f"  âœ… {pkg}")
        except Exception as e:
            print(f"  âš ï¸  {pkg} â€” install failed ({e}), will try to continue")

print("ğŸ“¦ Installing packages...")
pip_install(
    "geopandas",
    "rasterio",
    "rasterstats",
    "shapely",
    "fiona",
    "pyproj",
    "richdem",
    "numpy",
    "pandas",
    "scipy",
    "scikit-learn",
    "statsmodels",
    "seaborn",
    "plotly",
    "matplotlib",
    "mapclassify",
    "contextily",
    "joypy",
    "xarray",
    "rioxarray",
    "earthpy",
    "tqdm",
    "openpyxl",
)

print("\nğŸ“š Importing libraries...")

# â”€â”€ STANDARD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import warnings
import traceback
import zipfile
import json
from pathlib import Path
from tqdm import tqdm

warnings.filterwarnings('ignore')

# â”€â”€ GEOSPATIAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import geopandas as gpd
import rasterio
from rasterio.transform import rowcol, xy
from rasterio.warp import calculate_default_transform, reproject, Resampling
from rasterio.mask import mask as rio_mask
from rasterio.features import geometry_mask
import rasterio.plot
import fiona
from shapely.geometry import (Point, LineString, MultiLineString,
                               Polygon, MultiPolygon, box)
from shapely.ops import unary_union, linemerge
from pyproj import CRS, Transformer
from rasterstats import zonal_stats

# â”€â”€ RICHDEM (optional, graceful fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import richdem as rd
    RICHDEM_OK = True
    print("  âœ… richdem available")
except ImportError:
    RICHDEM_OK = False
    print("  âš ï¸  richdem not available â€” slope/aspect computed via numpy")

# â”€â”€ NUMERICAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import pandas as pd
from scipy import stats
from scipy.spatial.distance import cdist
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

# â”€â”€ SKLEARN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# â”€â”€ STATSMODELS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# â”€â”€ VISUALIZATION â€” MATPLOTLIB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.ticker as mticker
import matplotlib.colors as mcolors
from matplotlib.colors import LightSource, LinearSegmentedColormap, Normalize
from matplotlib.patches import FancyArrowPatch, FancyBboxPatch
import matplotlib.patheffects as pe
from mpl_toolkits.axes_grid1 import make_axes_locatable
import seaborn as sns

# â”€â”€ VISUALIZATION â€” PLOTLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# â”€â”€ OPTIONAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import joypy
    JOYPY_OK = True
except ImportError:
    JOYPY_OK = False
    print("  âš ï¸  joypy not available â€” ridge plots skipped")

try:
    import earthpy.spatial as es
    EARTHPY_OK = True
except ImportError:
    EARTHPY_OK = False

try:
    import xarray as xr
    import rioxarray
    RIOXARRAY_OK = True
except ImportError:
    RIOXARRAY_OK = False

# â”€â”€ GLOBAL SETTINGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pd.set_option('display.max_columns', 30)
pd.set_option('display.width', 200)
pd.set_option('display.float_format', '{:.4f}'.format)
plt.rcParams.update({
    'figure.dpi': 150,
    'font.family': 'DejaVu Sans',
    'axes.labelsize': 11,
    'axes.titlesize': 13,
    'legend.fontsize': 10,
})

# â”€â”€ OUTPUT DIRECTORIES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUT_DIR      = "/content/morphometric_outputs/"
MAPS_DIR     = os.path.join(OUT_DIR, "maps/")
PLOTS_DIR    = os.path.join(OUT_DIR, "plots/")
TABLES_DIR   = os.path.join(OUT_DIR, "tables/")
SHAPES_DIR   = os.path.join(OUT_DIR, "shapefiles/")
REPORT_DIR   = os.path.join(OUT_DIR, "report/")

for d in [OUT_DIR, MAPS_DIR, PLOTS_DIR, TABLES_DIR, SHAPES_DIR, REPORT_DIR]:
    os.makedirs(d, exist_ok=True)

print("\nâœ… All libraries imported successfully.")
print(f"ğŸ“ Output directory: {OUT_DIR}")

# â”€â”€ VERSION REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n{'='*50}")
print(f"  geopandas  : {gpd.__version__}")
print(f"  rasterio   : {rasterio.__version__}")
print(f"  numpy      : {np.__version__}")
print(f"  pandas     : {pd.__version__}")
print(f"  plotly     : {__import__('plotly').__version__}")
print(f"{'='*50}")

import subprocess, sys

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  SECTION 2 â€” DATA PATHS (Updated for 3-Pour-Point Pravra Basin Run)        â”‚
# â”‚                                                                             â”‚
# â”‚  Subbasins : Pravrabasin.shp  â† contains 5 polygons as confirmed by DBF   â”‚
# â”‚              If you have a 3-polygon delineation, replace path with:       â”‚
# â”‚              r"/content/watershed_data/pravra3.shp"                        â”‚
# â”‚  Pour Pts  : Pourpoints_3.shp â† 3 points confirmed âœ…                     â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# â”€â”€ â–¼â–¼â–¼  EDIT THESE PATHS  â–¼â–¼â–¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_PATHS = {
    "dem"              : r"/content/watershed_data/Filled DEM.tif",
    # â”€â”€ SUBBASINS: Using Pravrabasin.shp (5 polygons found in zip).
    #    Replace path below with pravra3.shp once you have the 3-polygon version.
    "subbasins"        : r"/content/watershed_data/Pravrabasin.shp",
    # â”€â”€ STREAMS: SteamOrder.shp is the polyline stream-order layer
    "streams"          : r"/content/watershed_data/SteamOrder.shp",
    "stream_order_shp" : r"/content/watershed_data/SteamOrder.shp",
    "flow_dir"         : r"/content/watershed_data/Flow Direction.tif",
    "flow_acc"         : r"/content/watershed_data/FlowAccumilation.tif",
    # â”€â”€ POUR POINTS: Updated from old Pourpoints-Pravrabasin.shp â†’ Pourpoints_3.shp
    "pour_points"      : r"/content/watershed_data/Pourpoints_3.shp",
}
# â”€â”€ â–²â–²â–²  EDIT ABOVE  â–²â–²â–² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# N_SUBBASINS is now set dynamically from the actual shapefile (see load step below).
# Override here ONLY if you want a strict assertion check:
#   N_SUBBASINS = 3    â† set to 3 when pravra3.shp (3-polygon file) is ready
#   N_SUBBASINS = 5    â† current Pravrabasin.shp has 5 polygons
N_SUBBASINS = None   # None = auto-detect from shapefile (recommended)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HELPER FUNCTIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_utm_epsg(lon, lat):
    """Return appropriate UTM EPSG code for a given lon/lat."""
    zone = int((lon + 180) / 6) + 1
    if lat >= 0:
        return f"EPSG:326{zone:02d}"
    else:
        return f"EPSG:327{zone:02d}"


def get_raster_info(path):
    """Return dict of raster metadata."""
    with rasterio.open(path) as src:
        return {
            "crs"        : src.crs,
            "res"        : src.res,
            "nodata"     : src.nodata,
            "shape"      : (src.height, src.width),
            "bounds"     : src.bounds,
            "dtype"      : src.dtypes[0],
            "count"      : src.count,
            "transform"  : src.transform,
        }


def reproject_raster(src_path, dst_path, target_crs):
    """Reproject a raster to target CRS and save."""
    with rasterio.open(src_path) as src:
        transform, width, height = calculate_default_transform(
            src.crs, target_crs, src.width, src.height, *src.bounds
        )
        kwargs = src.meta.copy()
        kwargs.update({
            'crs'       : target_crs,
            'transform' : transform,
            'width'     : width,
            'height'    : height,
        })
        with rasterio.open(dst_path, 'w', **kwargs) as dst:
            for i in range(1, src.count + 1):
                reproject(
                    source=rasterio.band(src, i),
                    destination=rasterio.band(dst, i),
                    src_transform=src.transform,
                    src_crs=src.crs,
                    dst_transform=transform,
                    dst_crs=target_crs,
                    resampling=Resampling.bilinear,
                )
    return dst_path


def fix_geometries(gdf, layer_name="layer"):
    """Fix invalid geometries and remove nulls."""
    before = len(gdf)
    gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.notna()].copy()
    gdf['geometry'] = gdf['geometry'].apply(
        lambda g: g.buffer(0) if not g.is_valid else g
    )
    gdf = gdf[gdf.geometry.is_valid].copy()
    print(f"  {layer_name}: {before} â†’ {len(gdf)} features (after geometry fix)")
    return gdf.reset_index(drop=True)


def explode_multipart(gdf, layer_name="layer"):
    """Explode multipart geometries to single-part."""
    before = len(gdf)
    gdf = gdf.explode(index_parts=False).reset_index(drop=True)
    if len(gdf) != before:
        print(f"  {layer_name}: Exploded multipart â†’ {len(gdf)} parts")
    return gdf


def snap_pour_points(pour_pts_gdf, flow_acc_path, snap_distance_m=300):
    """
    Snap pour points to the highest flow accumulation cell
    within snap_distance_m (in metres, projected CRS assumed).
    Returns GeoDataFrame with snapped geometries.
    """
    with rasterio.open(flow_acc_path) as src:
        fa_data  = src.read(1).astype(float)
        nodata   = src.nodata if src.nodata is not None else -9999
        fa_data[fa_data == nodata] = np.nan
        transform = src.transform
        res        = src.res[0]  # metres per pixel

    snap_cells = int(snap_distance_m / res)
    snapped_pts = []

    for idx, row in pour_pts_gdf.iterrows():
        px_c, px_r = ~transform * (row.geometry.x, row.geometry.y)
        px_c, px_r = int(px_c), int(px_r)

        r0 = max(0, px_r - snap_cells)
        r1 = min(fa_data.shape[0], px_r + snap_cells + 1)
        c0 = max(0, px_c - snap_cells)
        c1 = min(fa_data.shape[1], px_c + snap_cells + 1)

        window = fa_data[r0:r1, c0:c1]
        if np.all(np.isnan(window)):
            snapped_pts.append(row.geometry)
            continue

        local_max = np.nanargmax(window)
        local_r, local_c = np.unravel_index(local_max, window.shape)
        global_r = r0 + local_r
        global_c = c0 + local_c

        snap_x, snap_y = xy(transform, global_r, global_c)
        snapped_pts.append(Point(snap_x, snap_y))

    result = pour_pts_gdf.copy()
    result['geometry']       = snapped_pts
    result['snap_distance_m'] = [
        row.geometry.distance(snapped_pts[i])
        for i, (_, row) in enumerate(pour_pts_gdf.iterrows())
    ]
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  LOAD & VALIDATE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("=" * 60)
print("SECTION 2 â€” DATA LOADING & PREPROCESSING")
print("=" * 60)

# â”€â”€ 1. Load DEM info first to determine UTM zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[1/6] Reading DEM metadata...")
assert os.path.exists(DATA_PATHS['dem']), f"DEM not found: {DATA_PATHS['dem']}"
dem_info = get_raster_info(DATA_PATHS['dem'])
print(f"  CRS      : {dem_info['crs']}")
print(f"  Res      : {dem_info['res']} m")
print(f"  Shape    : {dem_info['shape']}")
print(f"  Bounds   : {dem_info['bounds']}")
print(f"  No-data  : {dem_info['nodata']}")

# Determine if geographic or projected
src_crs = CRS.from_user_input(dem_info['crs'])
if src_crs.is_geographic:
    # Compute centroid lon/lat for UTM zone
    b = dem_info['bounds']
    cen_lon = (b.left + b.right) / 2
    cen_lat = (b.bottom + b.top) / 2
    UTM_EPSG = detect_utm_epsg(cen_lon, cen_lat)
    print(f"  DEM is geographic â†’ will reproject to {UTM_EPSG}")
    NEEDS_REPROJECT = True
else:
    UTM_EPSG = str(dem_info['crs'])
    print(f"  DEM is already projected: {UTM_EPSG}")
    NEEDS_REPROJECT = False

TARGET_CRS = CRS.from_epsg(int(UTM_EPSG.split(":")[1]))

# â”€â”€ 2. Reproject rasters if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[2/6] Reprojecting rasters...")
RASTER_KEYS = ['dem', 'flow_dir', 'flow_acc']
RASTERS = {}

for key in RASTER_KEYS:
    src_path = DATA_PATHS[key]
    assert os.path.exists(src_path), f"Missing: {src_path}"
    info = get_raster_info(src_path)
    if NEEDS_REPROJECT and CRS.from_user_input(info['crs']).is_geographic:
        dst_path = os.path.join(OUT_DIR, f"{key}_utm.tif")
        reproject_raster(src_path, dst_path, TARGET_CRS)
        RASTERS[key] = dst_path
        print(f"  âœ… Reprojected {key}")
    else:
        RASTERS[key] = src_path
        print(f"  âœ… {key} OK (already projected)")

# Optional stream order raster
if os.path.exists(DATA_PATHS.get('stream_order_raster', '')):
    so_path = DATA_PATHS['stream_order_raster']
    so_info = get_raster_info(so_path)
    if NEEDS_REPROJECT and CRS.from_user_input(so_info['crs']).is_geographic:
        dst = os.path.join(OUT_DIR, "stream_order_utm.tif")
        reproject_raster(so_path, dst, TARGET_CRS)
        RASTERS['stream_order_raster'] = dst
    else:
        RASTERS['stream_order_raster'] = so_path

# â”€â”€ 3. Load & validate vector layers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[3/6] Loading vector layers...")

# Subbasins
gdf_sub = gpd.read_file(DATA_PATHS['subbasins'])
gdf_sub = fix_geometries(gdf_sub, "subbasins")
gdf_sub = gdf_sub.to_crs(UTM_EPSG)

# â”€â”€ Dynamic N_SUBBASINS: auto-detect from the loaded shapefile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if N_SUBBASINS is None:
    N_SUBBASINS = len(gdf_sub)
    print(f"  â„¹ï¸  N_SUBBASINS auto-detected: {N_SUBBASINS} polygons in {DATA_PATHS['subbasins']}")
else:
    if len(gdf_sub) != N_SUBBASINS:
        print(f"  âš ï¸  Warning: Expected {N_SUBBASINS} subbasins but shapefile has {len(gdf_sub)}.")
        print(f"       Using actual count: {len(gdf_sub)}")
        print(f"       â†’ If you want exactly 3 subbasins, replace subbasins path with pravra3.shp")
        N_SUBBASINS = len(gdf_sub)
    else:
        print(f"  âœ… Subbasin count verified: {N_SUBBASINS}")

print(f"  âœ… Subbasins: {len(gdf_sub)} | CRS: {gdf_sub.crs}")

# Ensure unique basin ID â€” try to detect from existing columns
if 'basin_id' not in gdf_sub.columns:
    # Try to use 'name' column if present (Pravrabasin.shp has 'name' field)
    if 'name' in gdf_sub.columns and gdf_sub['name'].notna().all():
        # Extract subbasin name from the 'AreaSqkm' column which has "Subbasin-X"
        # Try AreaSqkm field first (Pravrabasin.shp stores subbasin names there)
        if 'AreaSqkm' in gdf_sub.columns:
            names = gdf_sub['AreaSqkm'].astype(str).str.extract(r'(Subbasin-\d+)')[0]
            if names.notna().sum() == len(gdf_sub):
                gdf_sub['basin_id'] = names
            else:
                gdf_sub['basin_id'] = [f"SB{i+1}" for i in range(len(gdf_sub))]
        else:
            gdf_sub['basin_id'] = [f"SB{i+1}" for i in range(len(gdf_sub))]
    else:
        gdf_sub['basin_id'] = [f"SB{i+1}" for i in range(len(gdf_sub))]
print(f"  Basin IDs: {gdf_sub['basin_id'].tolist()}")

# Streams
gdf_streams = gpd.read_file(DATA_PATHS['streams'])
gdf_streams = fix_geometries(gdf_streams, "streams")
gdf_streams = explode_multipart(gdf_streams, "streams")
gdf_streams = gdf_streams.to_crs(UTM_EPSG)
print(f"  âœ… Streams: {len(gdf_streams)} segments | CRS: {gdf_streams.crs}")

# Stream order shapefile
gdf_so = gpd.read_file(DATA_PATHS['stream_order_shp'])
gdf_so = fix_geometries(gdf_so, "stream_order")
gdf_so = explode_multipart(gdf_so, "stream_order")
gdf_so = gdf_so.to_crs(UTM_EPSG)

# Detect stream order column
ORDER_COL = 'grid_code'  # SteamOrder.shp uses 'grid_code' for Strahler order (confirmed in DBF)

if ORDER_COL is None:
    raise ValueError(
        f"Cannot detect stream order column. Columns: {gdf_so.columns.tolist()}\n"
        "Please set ORDER_COL manually below."
    )
print(f"  âœ… Stream order col detected: '{ORDER_COL}' "
      f"| Orders: {sorted(gdf_so[ORDER_COL].unique())}")

gdf_so[ORDER_COL] = gdf_so[ORDER_COL].astype(int)
MAX_ORDER = int(gdf_so[ORDER_COL].max())

# Pour points (optional but important for snapping)
POUR_POINTS_OK = False
if os.path.exists(DATA_PATHS.get('pour_points', '')):
    gdf_pp = gpd.read_file(DATA_PATHS['pour_points'])
    gdf_pp = gdf_pp.to_crs(UTM_EPSG)
    print(f"  âœ… Pour points: {len(gdf_pp)}")
    print("  Snapping pour points to max flow accumulation...")
    gdf_pp = snap_pour_points(gdf_pp, RASTERS['flow_acc'], snap_distance_m=300)
    print(f"  Snap distances (m): {gdf_pp['snap_distance_m'].round(1).tolist()}")
    gdf_pp.to_file(os.path.join(SHAPES_DIR, "pour_points_snapped.shp"))
    POUR_POINTS_OK = True
else:
    gdf_pp = None
    print("  âš ï¸  Pour points file not found â€” skipping snap")

# â”€â”€ 4. Validate DEM resolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n[4/6] Validating DEM resolution...")
dem_info_utm = get_raster_info(RASTERS['dem'])
res_x, res_y = dem_info_utm['res']
if 20 <= res_x <= 35:
    print(f"  âœ… DEM resolution: {res_x:.1f} x {res_y:.1f} m â‰ˆ 30 m SRTM âœ“")
else:
    print(f"  âš ï¸  DEM resolution: {res_x:.1f} x {res_y:.1f} m (not standard 30 m â€” continuing anyway)")

# â”€â”€ 5. Read raster arrays into memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[5/6] Reading raster arrays...")

with rasterio.open(RASTERS['dem']) as src:
    DEM_ARR       = src.read(1).astype(np.float32)
    DEM_NODATA    = src.nodata if src.nodata is not None else -9999.0
    DEM_TRANSFORM = src.transform
    DEM_CRS       = src.crs
    DEM_BOUNDS    = src.bounds
    DEM_RES       = src.res[0]
    DEM_ARR[DEM_ARR == DEM_NODATA] = np.nan

with rasterio.open(RASTERS['flow_dir']) as src:
    FDIR_ARR    = src.read(1).astype(np.float32)
    FDIR_NODATA = src.nodata if src.nodata is not None else -9999.0
    FDIR_ARR[FDIR_ARR == FDIR_NODATA] = np.nan

with rasterio.open(RASTERS['flow_acc']) as src:
    FACC_ARR    = src.read(1).astype(np.float32)
    FACC_NODATA = src.nodata if src.nodata is not None else -9999.0
    FACC_ARR[FACC_ARR == FACC_NODATA] = np.nan

print(f"  DEM  shape: {DEM_ARR.shape} | min={np.nanmin(DEM_ARR):.1f} max={np.nanmax(DEM_ARR):.1f} m")
print(f"  FDIR shape: {FDIR_ARR.shape}")
print(f"  FACC shape: {FACC_ARR.shape}")

# â”€â”€ 6. Compute slope & aspect if not provided â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[6/6] Computing slope and aspect...")

def compute_slope_aspect_numpy(dem, res_m):
    """Compute slope (degrees) and aspect (degrees) using numpy gradient."""
    # Smooth first to reduce noise
    from scipy.ndimage import uniform_filter
    dem_sm = np.where(np.isnan(dem), 0, dem)
    dz_dy, dz_dx = np.gradient(dem_sm, res_m, res_m)
    slope_rad = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))
    slope_deg = np.degrees(slope_rad)
    aspect_deg = np.degrees(np.arctan2(-dz_dx, dz_dy)) % 360
    slope_deg[np.isnan(dem)] = np.nan
    aspect_deg[np.isnan(dem)] = np.nan
    return slope_deg.astype(np.float32), aspect_deg.astype(np.float32)


if RICHDEM_OK:
    try:
        rda = rd.rdarray(np.where(np.isnan(DEM_ARR), -9999, DEM_ARR), no_data=-9999)
        rda.projection = DEM_CRS.to_wkt()
        rda.geotransform = (DEM_TRANSFORM.c, DEM_TRANSFORM.a, 0,
                            DEM_TRANSFORM.f, 0, DEM_TRANSFORM.e)
        SLOPE_ARR  = np.array(rd.TerrainAttribute(rda, attrib='slope_degrees')).astype(np.float32)
        ASPECT_ARR = np.array(rd.TerrainAttribute(rda, attrib='aspect')).astype(np.float32)
        SLOPE_ARR[np.isnan(DEM_ARR)]  = np.nan
        ASPECT_ARR[np.isnan(DEM_ARR)] = np.nan
        print("  âœ… Slope & aspect from richdem")
    except Exception as e:
        print(f"  âš ï¸  richdem failed ({e}) â€” using numpy")
        SLOPE_ARR, ASPECT_ARR = compute_slope_aspect_numpy(DEM_ARR, DEM_RES)
else:
    SLOPE_ARR, ASPECT_ARR = compute_slope_aspect_numpy(DEM_ARR, DEM_RES)
    print("  âœ… Slope & aspect from numpy gradient")

# Save slope & aspect to disk
def save_raster(arr, path, template_path):
    with rasterio.open(template_path) as src:
        meta = src.meta.copy()
    meta.update({'dtype': 'float32', 'nodata': -9999.0, 'count': 1})
    arr_save = np.where(np.isnan(arr), -9999.0, arr)
    with rasterio.open(path, 'w', **meta) as dst:
        dst.write(arr_save.astype(np.float32), 1)

save_raster(SLOPE_ARR,  os.path.join(OUT_DIR, "slope.tif"),  RASTERS['dem'])
save_raster(ASPECT_ARR, os.path.join(OUT_DIR, "aspect.tif"), RASTERS['dem'])
RASTERS['slope']  = os.path.join(OUT_DIR, "slope.tif")
RASTERS['aspect'] = os.path.join(OUT_DIR, "aspect.tif")

# â”€â”€ HILLSHADE (used as background in all maps) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("  Computing hillshade for map backgrounds...")
ls = LightSource(azdeg=315, altdeg=45)
dem_filled = np.where(np.isnan(DEM_ARR), np.nanmean(DEM_ARR), DEM_ARR)
HILLSHADE   = ls.hillshade(dem_filled, vert_exag=1.5, dx=DEM_RES, dy=DEM_RES)
HILLSHADE[np.isnan(DEM_ARR)] = np.nan
print("  âœ… Hillshade computed")

# â”€â”€ SPATIAL INDEX (for fast spatial joins) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâœ… SECTION 2 complete.")
print(f"  Subbasins    : {len(gdf_sub)}")
print(f"  Stream segs  : {len(gdf_streams)}")
print(f"  Stream orders: {sorted(gdf_so[ORDER_COL].unique())}")
print(f"  UTM CRS      : {UTM_EPSG}")
print(f"  DEM range    : {np.nanmin(DEM_ARR):.1f} â€“ {np.nanmax(DEM_ARR):.1f} m")
print(f"  Slope range  : {np.nanmin(SLOPE_ARR):.1f}Â° â€“ {np.nanmax(SLOPE_ARR):.1f}Â°")

"""
=============================================================================
SECTION 3 â€” MORPHOMETRIC PARAMETER CALCULATION
=============================================================================
Computes all linear, areal, and relief morphometric parameters
per subbasin following Horton (1945), Strahler (1952, 1964),
Schumm (1956), and Miller (1953).
=============================================================================
"""

print("=" * 60)
print("SECTION 3 â€” MORPHOMETRIC PARAMETER CALCULATION")
print("=" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. LINEAR ASPECTS  (stream order statistics)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[A] Computing Linear Aspects...")

def compute_linear_aspects(gdf_streams_clipped, order_col, basin_id):
    """
    Compute stream order statistics for one subbasin.
    Returns per-order DataFrame and summary ratios.
    """
    rows = []
    orders = sorted(gdf_streams_clipped[order_col].unique())

    for u in orders:
        segs = gdf_streams_clipped[gdf_streams_clipped[order_col] == u]
        nu   = len(segs)
        lu   = segs.geometry.length.sum()
        lsm  = lu / nu if nu > 0 else 0
        rows.append({'basin_id': basin_id, 'order': u,
                     'Nu': nu, 'Lu': lu, 'Lsm': lsm})

    df = pd.DataFrame(rows).set_index('order')

    # Bifurcation ratio Rb = Nu / Nu+1
    df['Rb'] = np.nan
    for i in range(len(df) - 1):
        o1, o2 = orders[i], orders[i+1]
        if df.loc[o2, 'Nu'] > 0:
            df.loc[o1, 'Rb'] = df.loc[o1, 'Nu'] / df.loc[o2, 'Nu']

    # Stream length ratio RL = Lsm(u) / Lsm(u-1)
    df['RL'] = np.nan
    for i in range(1, len(df)):
        o_prev, o_curr = orders[i-1], orders[i]
        if df.loc[o_prev, 'Lsm'] > 0:
            df.loc[o_curr, 'RL'] = df.loc[o_curr, 'Lsm'] / df.loc[o_prev, 'Lsm']

    # Mean bifurcation ratio (arithmetic)
    Rb_vals = df['Rb'].dropna()
    Rbm = Rb_vals.mean() if len(Rb_vals) > 0 else np.nan

    # Weighted mean bifurcation ratio (Strahler, 1957)
    wRbm = np.nan
    if len(Rb_vals) > 0:
        weights = []
        for i in range(len(orders) - 1):
            o1, o2 = orders[i], orders[i+1]
            if not np.isnan(df.loc[o1, 'Rb']):
                weights.append(df.loc[o1, 'Nu'] + df.loc[o2, 'Nu'])
            else:
                weights.append(0)
        wts = np.array(weights)
        rb_wts = Rb_vals.values
        if wts.sum() > 0:
            wRbm = np.average(rb_wts, weights=wts[:len(rb_wts)])

    return df.reset_index(), Rbm, wRbm


# Spatial join: streams to subbasins
gdf_so_sub = gpd.sjoin(
    gdf_so[[ORDER_COL, 'geometry']],
    gdf_sub[['basin_id', 'geometry']],
    how='left', predicate='within'
)
# Fallback: intersects for streams spanning boundaries
gdf_so_inter = gpd.sjoin(
    gdf_so[[ORDER_COL, 'geometry']],
    gdf_sub[['basin_id', 'geometry']],
    how='left', predicate='intersects'
)
gdf_so_sub = gdf_so_sub.dropna(subset=['basin_id'])
if len(gdf_so_sub) == 0:
    gdf_so_sub = gdf_so_inter.dropna(subset=['basin_id'])

LINEAR_PER_ORDER = {}   # basin_id â†’ DataFrame
LINEAR_SUMMARY   = []   # one row per basin

for bid in gdf_sub['basin_id']:
    segs = gdf_so_sub[gdf_so_sub['basin_id'] == bid]
    if len(segs) == 0:
        print(f"  âš ï¸  No stream segments found for basin {bid}")
        continue
    df_lin, Rbm, wRbm = compute_linear_aspects(segs, ORDER_COL, bid)
    LINEAR_PER_ORDER[bid] = df_lin

    total_N = df_lin['Nu'].sum()
    total_L = df_lin['Lu'].sum()
    max_ord = df_lin['order'].max()

    LINEAR_SUMMARY.append({
        'basin_id'       : bid,
        'total_streams_N': total_N,
        'total_length_m' : total_L,
        'max_order'      : max_ord,
        'Rbm'            : round(Rbm, 4),
        'wRbm'           : round(wRbm, 4) if not np.isnan(wRbm) else np.nan,
    })
    print(f"  {bid}: {total_N} streams | max order {max_ord} | Rbm={Rbm:.3f}")

df_linear_summary = pd.DataFrame(LINEAR_SUMMARY).set_index('basin_id')
print("\n  Stream Order Summary (all basins):")
for bid, df in LINEAR_PER_ORDER.items():
    print(f"\n  [{bid}]")
    print(df[['order','Nu','Lu','Lsm','Rb','RL']].to_string(index=False))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. AREAL ASPECTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[B] Computing Areal Aspects...")

def longest_flow_path(basin_geom, facc_arr, transform, res_m):
    """
    Approximate basin length (Lb) using the longest flow path concept:
    distance from centroid to pour point approximated as sqrt(A / 1.128)
    (Hack, 1957) as fallback.  Actual longest flow path would require
    full D8 tracing â€” approximation is acceptable for published studies.
    """
    area = basin_geom.area          # mÂ²
    lb   = np.sqrt(area / 1.128)   # Hack approximation
    return lb


AREAL = []

for _, row in gdf_sub.iterrows():
    bid   = row['basin_id']
    geom  = row.geometry

    A  = geom.area          # mÂ²
    P  = geom.length        # m
    Lb = longest_flow_path(geom, FACC_ARR, DEM_TRANSFORM, DEM_RES)

    # Streams inside basin
    segs = gdf_so_sub[gdf_so_sub['basin_id'] == bid]
    total_stream_length = segs.geometry.length.sum() if len(segs) > 0 else 0
    Nu_total = len(segs)

    # ----- parameters -----
    A_km2   = A   / 1e6
    P_km    = P   / 1e3
    Lb_km   = Lb  / 1e3
    L_km    = total_stream_length / 1e3

    Dd = L_km  / A_km2 if A_km2 > 0 else np.nan   # Drainage density  [km/kmÂ²]
    Fs = Nu_total / A_km2 if A_km2 > 0 else np.nan # Stream frequency  [streams/kmÂ²]
    T  = Nu_total / P_km  if P_km  > 0 else np.nan # Texture ratio
    Ff = A_km2   / (Lb_km**2)     if Lb_km > 0 else np.nan  # Form factor (Horton,1932)
    Re = (2 / Lb_km) * np.sqrt(A_km2 / np.pi) if Lb_km > 0 else np.nan  # Elongation ratio
    Rc = (4 * np.pi * A_km2) / (P_km**2)  if P_km > 0 else np.nan       # Circularity ratio
    Cc = P_km / (2 * np.sqrt(np.pi * A_km2)) if A_km2 > 0 else np.nan   # Compactness coeff
    Lg = 1 / (2 * Dd)   if Dd and Dd > 0 else np.nan   # Length of overland flow
    C  = 1 / Dd          if Dd and Dd > 0 else np.nan   # Constant of channel maintenance

    AREAL.append({
        'basin_id'              : bid,
        'Area_km2'              : round(A_km2, 4),
        'Perimeter_km'          : round(P_km, 4),
        'Basin_Length_km'       : round(Lb_km, 4),
        'Total_Stream_Length_km': round(L_km, 4),
        'Stream_Count'          : Nu_total,
        'Drainage_Density_Dd'   : round(Dd, 4) if not np.isnan(Dd) else np.nan,
        'Stream_Frequency_Fs'   : round(Fs, 4) if not np.isnan(Fs) else np.nan,
        'Texture_Ratio_T'       : round(T,  4) if not np.isnan(T)  else np.nan,
        'Form_Factor_Ff'        : round(Ff, 4) if not np.isnan(Ff) else np.nan,
        'Elongation_Ratio_Re'   : round(Re, 4) if not np.isnan(Re) else np.nan,
        'Circularity_Ratio_Rc'  : round(Rc, 4) if not np.isnan(Rc) else np.nan,
        'Compactness_Cc'        : round(Cc, 4) if not np.isnan(Cc) else np.nan,
        'LengthOverlandFlow_Lg' : round(Lg, 4) if not np.isnan(Lg) else np.nan,
        'ChannelMaintenance_C'  : round(C,  4) if not np.isnan(C)  else np.nan,
    })

    print(f"  {bid}: A={A_km2:.2f} kmÂ² | Dd={Dd:.3f} km/kmÂ² | "
          f"Re={Re:.3f} | Rc={Rc:.3f} | Ff={Ff:.3f}")

df_areal = pd.DataFrame(AREAL).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. RELIEF ASPECTS  (DEM zonal statistics)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[C] Computing Relief Aspects...")

def hypsometric_integral(dem_clipped):
    """
    Compute hypsometric integral (HI) = (mean_elev - min_elev) / (max_elev - min_elev)
    Also returns arrays for hypsometric curve: (relative_area, relative_elevation)
    """
    vals = dem_clipped[~np.isnan(dem_clipped)].flatten()
    if len(vals) < 10:
        return np.nan, None, None
    mn, mx, mu = vals.min(), vals.max(), vals.mean()
    rng = mx - mn
    if rng == 0:
        return np.nan, None, None
    HI = (mu - mn) / rng
    # Curve: relative elevation h/H vs relative area a/A
    thresholds   = np.percentile(vals, np.linspace(0, 100, 101))
    rel_elev     = (thresholds - mn) / rng          # h/H  (0â†’1)
    rel_area     = 1 - np.linspace(0, 1, 101)       # a/A  (1â†’0)
    return HI, rel_area, rel_elev


def terrain_ruggedness_index(dem_arr):
    """
    TRI (Riley et al., 1999): mean absolute difference from center cell
    to 8 neighbours.
    """
    from scipy.ndimage import generic_filter
    def _tri_kernel(x):
        centre = x[4]
        if np.isnan(centre):
            return np.nan
        diffs = x - centre
        diffs[4] = 0
        valid = diffs[~np.isnan(diffs)]
        return np.sqrt(np.sum(valid**2)) if len(valid) > 0 else np.nan
    tri = generic_filter(dem_arr.astype(float), _tri_kernel, size=3, mode='reflect')
    tri[np.isnan(dem_arr)] = np.nan
    return tri


def melton_ruggedness(h_m, a_km2):
    """Melton (1965) ruggedness index MRN = H / sqrt(A)."""
    return h_m / np.sqrt(a_km2) if a_km2 > 0 else np.nan


# Compute TRI once for full DEM
print("  Computing TRI (this may take 30â€“60 sec on large DEMs)...")
TRI_ARR = terrain_ruggedness_index(DEM_ARR)
save_raster(TRI_ARR, os.path.join(OUT_DIR, "tri.tif"), RASTERS['dem'])

RELIEF   = []
HYPS     = {}   # basin_id â†’ (rel_area, rel_elev)

for _, row in gdf_sub.iterrows():
    bid  = row['basin_id']
    geom = [row.geometry.__geo_interface__]

    # Mask DEM to subbasin
    with rasterio.open(RASTERS['dem']) as src:
        try:
            arr_masked, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
            dem_clip = arr_masked[0].astype(np.float32)
            dem_clip[dem_clip == src.nodata] = np.nan
        except Exception:
            dem_clip = DEM_ARR.copy()

    # Mask slope to subbasin
    with rasterio.open(RASTERS['slope']) as src:
        try:
            s_masked, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
            slope_clip = s_masked[0].astype(np.float32)
            slope_clip[slope_clip == -9999.0] = np.nan
        except Exception:
            slope_clip = SLOPE_ARR.copy()

    # Mask TRI
    with rasterio.open(os.path.join(OUT_DIR, "tri.tif")) as src:
        try:
            t_masked, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
            tri_clip = t_masked[0].astype(np.float32)
            tri_clip[tri_clip == -9999.0] = np.nan
        except Exception:
            tri_clip = TRI_ARR.copy()

    valid_dem   = dem_clip[~np.isnan(dem_clip)]
    valid_slope = slope_clip[~np.isnan(slope_clip)]
    valid_tri   = tri_clip[~np.isnan(tri_clip)]

    if len(valid_dem) == 0:
        print(f"  âš ï¸  {bid}: no valid DEM cells")
        continue

    elev_min  = float(valid_dem.min())
    elev_max  = float(valid_dem.max())
    elev_mean = float(valid_dem.mean())
    H         = elev_max - elev_min              # Basin relief (m)
    A_km2     = df_areal.loc[bid, 'Area_km2']
    Lb_km     = df_areal.loc[bid, 'Basin_Length_km']
    P_km      = df_areal.loc[bid, 'Perimeter_km']

    Rh  = H / (Lb_km * 1000) if Lb_km > 0 else np.nan   # Relief ratio
    Rr  = H / P_km            if P_km  > 0 else np.nan   # Relative relief
    Dd  = df_areal.loc[bid, 'Drainage_Density_Dd']
    Rn  = H * Dd / 1000       if not np.isnan(Dd) else np.nan  # Ruggedness number
    MRN = melton_ruggedness(H, A_km2)                           # Melton ruggedness

    # Hypsometric integral
    HI, rel_area, rel_elev = hypsometric_integral(dem_clip)
    if rel_area is not None:
        HYPS[bid] = (rel_area, rel_elev)

    # Slope statistics
    slope_mean = float(np.nanmean(valid_slope))
    slope_std  = float(np.nanstd(valid_slope))
    slope_skew = float(stats.skew(valid_slope))

    # TRI stats
    tri_mean = float(np.nanmean(valid_tri))

    RELIEF.append({
        'basin_id'         : bid,
        'Elev_Min_m'       : round(elev_min,  2),
        'Elev_Max_m'       : round(elev_max,  2),
        'Elev_Mean_m'      : round(elev_mean, 2),
        'Basin_Relief_H_m' : round(H,         2),
        'Relief_Ratio_Rh'  : round(Rh,        6) if not np.isnan(Rh) else np.nan,
        'Relative_Relief'  : round(Rr,        4) if not np.isnan(Rr) else np.nan,
        'Ruggedness_Rn'    : round(Rn,        4) if not np.isnan(Rn) else np.nan,
        'Melton_MRN'       : round(MRN,       4) if not np.isnan(MRN) else np.nan,
        'Hypsometric_HI'   : round(HI,        4) if not np.isnan(HI) else np.nan,
        'Slope_Mean_deg'   : round(slope_mean, 3),
        'Slope_Std_deg'    : round(slope_std,  3),
        'Slope_Skewness'   : round(slope_skew, 4),
        'TRI_Mean'         : round(tri_mean,   3),
    })

    print(f"  {bid}: H={H:.0f}m | Rh={Rh:.5f} | HI={HI:.3f} | "
          f"Rn={Rn:.3f} | Slope_mean={slope_mean:.2f}Â°")

df_relief = pd.DataFrame(RELIEF).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. MASTER MORPHOMETRIC TABLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[D] Assembling master morphometric table...")

df_master = df_areal.join(df_relief, how='left')
df_master = df_master.join(df_linear_summary, how='left')

# Add stream order per-basin summary
for bid in gdf_sub['basin_id']:
    if bid in LINEAR_PER_ORDER:
        df_lin = LINEAR_PER_ORDER[bid]
        for _, r in df_lin.iterrows():
            col = f"Nu_order{int(r['order'])}"
            df_master.loc[bid, col] = r['Nu']
            col = f"Lu_order{int(r['order'])}_km"
            df_master.loc[bid, col] = round(r['Lu'] / 1000, 4)

# â”€â”€ Interpretation flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def interpret_elongation(Re):
    if pd.isna(Re):       return "Unknown"
    if Re >= 0.9:         return "Circular"
    if Re >= 0.8:         return "Oval"
    if Re >= 0.7:         return "Less Elongated"
    if Re >= 0.5:         return "Elongated"
    return "More Elongated"

def interpret_circularity(Rc):
    if pd.isna(Rc):      return "Unknown"
    if Rc >= 0.75:       return "Circular/Young"
    if Rc >= 0.50:       return "Intermediate"
    return "Elongated/Old"

def interpret_HI(HI):
    if pd.isna(HI):      return "Unknown"
    if HI > 0.60:        return "Monadnock (Young/Convex)"
    if HI > 0.35:        return "Mature (Equilibrium)"
    return "Peneplain (Old/Concave)"

df_master['Shape_Class']    = df_master['Elongation_Ratio_Re'].apply(interpret_elongation)
df_master['Circ_Class']     = df_master['Circularity_Ratio_Rc'].apply(interpret_circularity)
df_master['Hyps_Class']     = df_master['Hypsometric_HI'].apply(interpret_HI)

# Save
csv_path = os.path.join(TABLES_DIR, "morphometric_master_table.csv")
df_master.to_csv(csv_path)
print(f"  âœ… Master table saved: {csv_path}")

print("\n" + "â”€"*60)
print("  MASTER MORPHOMETRIC TABLE (first 10 rows/all params):")
print("â”€"*60)
print(df_master.to_string())

"""
=============================================================================
SECTION 4 â€” PUBLICATION-GRADE MAPS
=============================================================================
Generates 9 maps, all with:
  â€¢ Hillshade background
  â€¢ DMS (Â°â€²â€³) grid
  â€¢ North arrow
  â€¢ Scale bar
  â€¢ Subbasin boundaries overlay
  â€¢ Stream network overlay
  â€¢ Colourbar / legend
  â€¢ Title

Maps produced:
  1. Elevation (DEM)
  2. Slope
  3. Aspect
  4. Flow Direction
  5. Flow Accumulation
  6. Stream Order (Strahler)
  7. Drainage Density
  8. Contour
  9. Pour Points (snapped) on DEM
=============================================================================
"""

print("=" * 60)
print("SECTION 4 â€” MAP GENERATION")
print("=" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  SHARED MAP UTILITIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from pyproj import Transformer as PyTransformer

# Transform from UTM â†’ WGS84 for grid labelling
_to_geo = PyTransformer.from_crs(UTM_EPSG, "EPSG:4326", always_xy=True)


def dd_to_dms(dd, is_lat=True):
    """Decimal degrees â†’ DMS string."""
    hemi = ("N" if dd >= 0 else "S") if is_lat else ("E" if dd >= 0 else "W")
    dd = abs(dd)
    deg = int(dd)
    mins_full = (dd - deg) * 60
    mins = int(mins_full)
    secs = (mins_full - mins) * 60
    return f"{deg}Â°{mins:02d}â€²{secs:04.1f}â€³{hemi}"


def get_dms_ticks(utm_extent, n=5):
    """
    Return (x_utm_ticks, x_labels, y_utm_ticks, y_labels)
    for DMS-formatted grid lines.
    utm_extent = (xmin, xmax, ymin, ymax) in UTM metres
    """
    xmin, xmax, ymin, ymax = utm_extent
    # Sample grid corners in geographic
    corners_utm = [
        (xmin, ymin), (xmax, ymin), (xmin, ymax), (xmax, ymax),
    ]
    lon_all, lat_all = [], []
    for xu, yu in corners_utm:
        lo, la = _to_geo.transform(xu, yu)
        lon_all.append(lo)
        lat_all.append(la)
    lon_min, lon_max = min(lon_all), max(lon_all)
    lat_min, lat_max = min(lat_all), max(lat_all)

    # Nicely spaced geographic ticks
    lon_ticks_geo = np.linspace(lon_min, lon_max, n)
    lat_ticks_geo = np.linspace(lat_min, lat_max, n)

    # Convert back to UTM for pyplot ticks
    from pyproj import Transformer as T2
    _to_utm = T2.from_crs("EPSG:4326", UTM_EPSG, always_xy=True)
    x_ticks_utm = [_to_utm.transform(lo, (lat_min + lat_max) / 2)[0] for lo in lon_ticks_geo]
    y_ticks_utm = [_to_utm.transform((lon_min + lon_max) / 2, la)[1] for la in lat_ticks_geo]

    x_labels = [dd_to_dms(lo, is_lat=False) for lo in lon_ticks_geo]
    y_labels = [dd_to_dms(la, is_lat=True)  for la in lat_ticks_geo]

    return x_ticks_utm, x_labels, y_ticks_utm, y_labels


def compute_utm_extent():
    """Return (xmin, xmax, ymin, ymax) from DEM bounds."""
    b = DEM_BOUNDS
    return b.left, b.right, b.bottom, b.top


def add_north_arrow(ax, x=0.96, y=0.94, size=0.045):
    """Add a north arrow to axes using annotation."""
    ax.annotate(
        '', xy=(x, y), xycoords='axes fraction',
        xytext=(x, y - size * 2),
        textcoords='axes fraction',
        arrowprops=dict(arrowstyle='->', color='black', lw=2),
        annotation_clip=False,
    )
    ax.text(x, y + 0.005, 'N', transform=ax.transAxes,
            ha='center', va='bottom', fontsize=13,
            fontweight='bold', color='black',
            path_effects=[pe.withStroke(linewidth=3, foreground='white')])


def add_scale_bar(ax, extent_m, frac=0.2, y_pos=0.04, x_pos=0.05):
    """
    Add a scale bar. extent_m = (xmin, xmax, ymin, ymax) in metres.
    """
    xmin, xmax = extent_m[0], extent_m[1]
    width_m = (xmax - xmin) * frac

    # Round to nice number
    magnitude = 10 ** np.floor(np.log10(width_m))
    width_m   = round(width_m / magnitude) * magnitude

    # In axes fraction
    total_m = xmax - xmin
    bar_frac = width_m / total_m

    label_km = f"{width_m/1000:.0f} km" if width_m >= 1000 else f"{width_m:.0f} m"

    ax.annotate(
        '', xy=(x_pos + bar_frac, y_pos), xycoords='axes fraction',
        xytext=(x_pos, y_pos), textcoords='axes fraction',
        arrowprops=dict(arrowstyle='<->', color='black', lw=2),
        annotation_clip=False,
    )
    ax.text(x_pos + bar_frac / 2, y_pos + 0.02,
            label_km, transform=ax.transAxes,
            ha='center', va='bottom', fontsize=9, color='black',
            path_effects=[pe.withStroke(linewidth=2, foreground='white')])


def apply_dms_grid(ax, utm_extent, n_ticks=5):
    """Apply DMS-labelled grid to axes."""
    x_ticks, x_labels, y_ticks, y_labels = get_dms_ticks(utm_extent, n=n_ticks)
    ax.set_xticks(x_ticks)
    ax.set_xticklabels(x_labels, rotation=25, ha='right', fontsize=7.5)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_labels, fontsize=7.5)
    ax.grid(True, linestyle='--', linewidth=0.4, color='grey', alpha=0.6)
    ax.tick_params(direction='in', top=True, right=True, length=4)


def base_axes(title, figsize=(11, 9)):
    """Create figure/axes with hillshade background."""
    fig, ax = plt.subplots(figsize=figsize)
    utm_extent = compute_utm_extent()
    # Hillshade background
    ax.imshow(
        HILLSHADE,
        extent=[utm_extent[0], utm_extent[1], utm_extent[2], utm_extent[3]],
        origin='upper', cmap='Greys', alpha=0.45,
        aspect='auto', zorder=0,
    )
    ax.set_xlim(utm_extent[0], utm_extent[1])
    ax.set_ylim(utm_extent[2], utm_extent[3])
    ax.set_title(title, fontsize=14, fontweight='bold', pad=10)
    ax.set_xlabel("Longitude", fontsize=10)
    ax.set_ylabel("Latitude",  fontsize=10)
    return fig, ax, utm_extent


def overlay_boundaries(ax, alpha_sub=0.9, alpha_str=0.5):
    """Overlay subbasin boundaries and stream network."""
    gdf_sub.boundary.plot(ax=ax, edgecolor='black', linewidth=1.2,
                          zorder=10, label='Subbasin boundary')
    if len(gdf_streams) > 0:
        gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.8,
                         alpha=alpha_str, zorder=9, label='Stream network')


def finalize_and_save(fig, ax, utm_extent, filename, n_ticks=5):
    """Apply grid, north arrow, scale bar, tight layout, save."""
    apply_dms_grid(ax, utm_extent, n_ticks)
    add_north_arrow(ax)
    add_scale_bar(ax, utm_extent)
    plt.tight_layout()
    out_path = os.path.join(MAPS_DIR, filename)
    fig.savefig(out_path, dpi=200, bbox_inches='tight')
    plt.close(fig)
    print(f"  âœ… Saved: {out_path}")
    return out_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MAP HELPER â€” raster_to_plot array
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def raster_extent():
    b = DEM_BOUNDS
    return [b.left, b.right, b.bottom, b.top]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  1. ELEVATION MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[1/9] Elevation map...")
fig, ax, utm_ext = base_axes("Elevation Map â€” SRTM 30 m DEM")
cmap_elev = plt.get_cmap('terrain')
im = ax.imshow(
    DEM_ARR,
    extent=raster_extent(), origin='upper',
    cmap=cmap_elev, alpha=0.75, zorder=1,
    vmin=np.nanpercentile(DEM_ARR, 2), vmax=np.nanpercentile(DEM_ARR, 98),
)
overlay_boundaries(ax)
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_label("Elevation (m)", fontsize=10)
ax.legend(loc='lower left', fontsize=8, framealpha=0.8)
finalize_and_save(fig, ax, utm_ext, "01_elevation.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  2. SLOPE MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[2/9] Slope map...")
fig, ax, utm_ext = base_axes("Slope Map (degrees)")
im = ax.imshow(
    SLOPE_ARR,
    extent=raster_extent(), origin='upper',
    cmap='YlOrRd', alpha=0.75, zorder=1,
    vmin=0, vmax=np.nanpercentile(SLOPE_ARR, 98),
)
overlay_boundaries(ax)
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_label("Slope (Â°)", fontsize=10)
finalize_and_save(fig, ax, utm_ext, "02_slope.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  3. ASPECT MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[3/9] Aspect map...")
fig, ax, utm_ext = base_axes("Aspect Map (degrees from North)")
cmap_aspect = plt.get_cmap('hsv')
im = ax.imshow(
    ASPECT_ARR,
    extent=raster_extent(), origin='upper',
    cmap=cmap_aspect, alpha=0.75, zorder=1,
    vmin=0, vmax=360,
)
overlay_boundaries(ax)
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_ticks([0, 45, 90, 135, 180, 225, 270, 315, 360])
cb.set_ticklabels(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW', 'N'])
cb.set_label("Aspect", fontsize=10)
finalize_and_save(fig, ax, utm_ext, "03_aspect.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  4. FLOW DIRECTION MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[4/9] Flow direction map...")
fig, ax, utm_ext = base_axes("Flow Direction Map (D8 encoding)")
# D8: 1=E,2=SE,4=S,8=SW,16=W,32=NW,64=N,128=NE
d8_labels = {1:'E',2:'SE',4:'S',8:'SW',16:'W',32:'NW',64:'N',128:'NE'}
unique_d8 = [v for v in sorted(d8_labels.keys()) if v in np.unique(FDIR_ARR[~np.isnan(FDIR_ARR)])]
colors_d8  = plt.cm.tab10(np.linspace(0, 1, 8))
d8_cmap    = mcolors.ListedColormap(colors_d8[:len(unique_d8)])
d8_bounds  = [unique_d8[0] - 0.5] + [v + 0.5 for v in unique_d8]
d8_norm    = mcolors.BoundaryNorm(d8_bounds, d8_cmap.N)

im = ax.imshow(
    FDIR_ARR,
    extent=raster_extent(), origin='upper',
    cmap=d8_cmap, norm=d8_norm, alpha=0.70, zorder=1,
)
overlay_boundaries(ax)
patches_d8 = [mpatches.Patch(color=colors_d8[i], label=d8_labels.get(unique_d8[i], str(unique_d8[i])))
              for i in range(len(unique_d8))]
ax.legend(handles=patches_d8, loc='lower left', fontsize=7,
          title='Flow Dir.', title_fontsize=8, framealpha=0.8, ncol=2)
finalize_and_save(fig, ax, utm_ext, "04_flow_direction.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  5. FLOW ACCUMULATION MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[5/9] Flow accumulation map...")
fig, ax, utm_ext = base_axes("Flow Accumulation Map (logâ‚â‚€ scale)")
fa_log = np.log10(np.where(FACC_ARR > 0, FACC_ARR, np.nan))
im = ax.imshow(
    fa_log,
    extent=raster_extent(), origin='upper',
    cmap='Blues', alpha=0.80, zorder=1,
)
overlay_boundaries(ax)
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_label("logâ‚â‚€(Flow Accum.)", fontsize=10)
finalize_and_save(fig, ax, utm_ext, "05_flow_accumulation.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  6. STREAM ORDER MAP (Strahler)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[6/9] Stream order map...")
fig, ax, utm_ext = base_axes("Strahler Stream Order Map")
# Hillshade already in base_axes
overlay_boundaries(ax, alpha_str=0)   # suppress default streams

orders_list = sorted(gdf_so[ORDER_COL].unique())
order_cmap  = plt.cm.get_cmap('plasma_r', len(orders_list))
order_colors = {o: order_cmap(i) for i, o in enumerate(orders_list)}
lw_map       = {o: 0.5 + (o - 1) * 0.6 for o in orders_list}

for o in orders_list:
    segs = gdf_so[gdf_so[ORDER_COL] == o]
    segs.plot(ax=ax, color=order_colors[o], linewidth=lw_map[o],
              zorder=5 + o, label=f"Order {o}")

gdf_sub.boundary.plot(ax=ax, edgecolor='black', linewidth=1.2, zorder=15)
ax.legend(loc='lower left', fontsize=8, framealpha=0.85, title='Strahler Order')
finalize_and_save(fig, ax, utm_ext, "06_stream_order.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  7. DRAINAGE DENSITY MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[7/9] Drainage density map...")
fig, ax, utm_ext = base_axes("Drainage Density Map (km/kmÂ²)")
gdf_dd = gdf_sub.merge(
    df_master[['Drainage_Density_Dd']].reset_index(),
    on='basin_id', how='left'
)
gdf_dd.plot(
    column='Drainage_Density_Dd', ax=ax,
    cmap='YlGnBu', legend=True, alpha=0.75, zorder=2,
    legend_kwds={'label': 'Drainage Density (km/kmÂ²)', 'shrink': 0.7},
    edgecolor='black', linewidth=1.0,
)
# Basin labels
for _, r in gdf_dd.iterrows():
    cx, cy = r.geometry.centroid.x, r.geometry.centroid.y
    ax.text(cx, cy, f"{r['basin_id']}\n{r['Drainage_Density_Dd']:.2f}",
            ha='center', va='center', fontsize=8, fontweight='bold',
            color='white',
            path_effects=[pe.withStroke(linewidth=2, foreground='black')])

gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.7, alpha=0.6, zorder=5)
finalize_and_save(fig, ax, utm_ext, "07_drainage_density.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  8. CONTOUR MAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[8/9] Contour map...")
fig, ax, utm_ext = base_axes("Topographic Contour Map")

b = DEM_BOUNDS
dem_range = np.nanmax(DEM_ARR) - np.nanmin(DEM_ARR)
interval  = max(10, round(dem_range / 20, -1))   # smart interval

x_c = np.linspace(b.left,   b.right,  DEM_ARR.shape[1])
y_c = np.linspace(b.bottom, b.top,    DEM_ARR.shape[0])[::-1]  # origin='upper'
XX, YY = np.meshgrid(x_c, y_c)

contour_levels = np.arange(
    round(np.nanmin(DEM_ARR) / interval) * interval,
    np.nanmax(DEM_ARR) + interval,
    interval,
)
major_levels = contour_levels[::4]

dem_filled_c = np.where(np.isnan(DEM_ARR), np.nanmean(DEM_ARR), DEM_ARR)
cs_minor = ax.contour(XX, YY, dem_filled_c, levels=contour_levels,
                       colors='saddlebrown', linewidths=0.4, alpha=0.5, zorder=3)
cs_major = ax.contour(XX, YY, dem_filled_c, levels=major_levels,
                       colors='saddlebrown', linewidths=1.0, alpha=0.85, zorder=4)
ax.clabel(cs_major, inline=True, fontsize=6.5, fmt='%d m')

overlay_boundaries(ax)
ax.text(0.02, 0.02, f"Contour interval: {interval:.0f} m",
        transform=ax.transAxes, fontsize=8, style='italic',
        bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
finalize_and_save(fig, ax, utm_ext, "08_contour.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  9. POUR POINTS ON DEM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("[9/9] Pour points map...")
fig, ax, utm_ext = base_axes("Pour Points (Snapped) on DEM")
im = ax.imshow(
    DEM_ARR,
    extent=raster_extent(), origin='upper',
    cmap='terrain', alpha=0.65, zorder=1,
    vmin=np.nanpercentile(DEM_ARR, 2), vmax=np.nanpercentile(DEM_ARR, 98),
)
overlay_boundaries(ax)

if POUR_POINTS_OK and gdf_pp is not None:
    gdf_pp.plot(ax=ax, color='red', markersize=80, zorder=20,
                label='Snapped pour points', marker='v', edgecolor='white', linewidth=0.8)
    for idx, r in gdf_pp.iterrows():
        label = str(r.get('basin_id', idx))
        ax.annotate(
            label,
            xy=(r.geometry.x, r.geometry.y),
            xytext=(5, 5), textcoords='offset points',
            fontsize=8, color='red', fontweight='bold',
            path_effects=[pe.withStroke(linewidth=2, foreground='white')],
        )

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_label("Elevation (m)", fontsize=10)
ax.legend(loc='lower left', fontsize=8, framealpha=0.85)
finalize_and_save(fig, ax, utm_ext, "09_pour_points.png")

print(f"\nâœ… All 9 maps saved to: {MAPS_DIR}")

"""
=============================================================================
SECTION 5 â€” STATISTICAL ANALYSIS
=============================================================================
Descriptive stats, correlation matrix, VIF, PCA, clustering.
=============================================================================
"""

print("=" * 60)
print("SECTION 5 â€” STATISTICAL ANALYSIS")
print("=" * 60)

# â”€â”€ Select numeric morphometric columns for analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STAT_COLS = [
    'Area_km2', 'Perimeter_km', 'Basin_Length_km',
    'Drainage_Density_Dd', 'Stream_Frequency_Fs', 'Texture_Ratio_T',
    'Form_Factor_Ff', 'Elongation_Ratio_Re', 'Circularity_Ratio_Rc',
    'Compactness_Cc', 'LengthOverlandFlow_Lg', 'ChannelMaintenance_C',
    'Basin_Relief_H_m', 'Relief_Ratio_Rh', 'Relative_Relief',
    'Ruggedness_Rn', 'Melton_MRN', 'Hypsometric_HI',
    'Slope_Mean_deg', 'TRI_Mean', 'Rbm',
]
# Keep only columns that actually exist in df_master
STAT_COLS = [c for c in STAT_COLS if c in df_master.columns]
df_stat   = df_master[STAT_COLS].copy().astype(float)
df_stat.dropna(axis=1, how='all', inplace=True)
STAT_COLS = df_stat.columns.tolist()

print(f"  Parameters for analysis: {len(STAT_COLS)}")
print(f"  Subbasins: {len(df_stat)}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. DESCRIPTIVE STATISTICS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[A] Descriptive Statistics...")

desc_extra = df_stat.agg([
    'mean', 'median', 'std',
    lambda x: (x.std()/x.mean()*100) if x.mean() != 0 else np.nan,  # CV%
    lambda x: float(stats.skew(x.dropna())),
    lambda x: float(stats.kurtosis(x.dropna())),
])
desc_extra.index = ['Mean', 'Median', 'Std', 'CV%', 'Skewness', 'Kurtosis']
desc_full = pd.concat([df_stat.describe(), desc_extra])

csv_path = os.path.join(TABLES_DIR, "descriptive_statistics.csv")
desc_full.to_csv(csv_path)
print(f"  âœ… Saved: {csv_path}")
print(desc_full.to_string())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. CORRELATION MATRICES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[B] Correlation Matrices (Pearson + Spearman)...")

# Pearson
corr_pearson  = df_stat.corr(method='pearson')
corr_spearman = df_stat.corr(method='spearman')

# Heatmap â€” Pearson
fig, axes = plt.subplots(1, 2, figsize=(20, 8))
for ax_corr, corr_mat, title in [
    (axes[0], corr_pearson,  "Pearson Correlation"),
    (axes[1], corr_spearman, "Spearman Correlation"),
]:
    mask = np.triu(np.ones_like(corr_mat, dtype=bool))
    sns.heatmap(
        corr_mat, mask=mask, ax=ax_corr,
        cmap='RdYlBu_r', center=0, vmin=-1, vmax=1,
        annot=True, fmt='.2f', annot_kws={'size': 7},
        linewidths=0.5, square=True, cbar_kws={'shrink': 0.7},
    )
    ax_corr.set_title(title, fontsize=13, fontweight='bold')
    ax_corr.set_xticklabels(ax_corr.get_xticklabels(), rotation=45,
                              ha='right', fontsize=7.5)
    ax_corr.set_yticklabels(ax_corr.get_yticklabels(), fontsize=7.5)

plt.tight_layout()
fig.savefig(os.path.join(PLOTS_DIR, "correlation_heatmap.png"), dpi=180, bbox_inches='tight')
plt.close(fig)
print("  âœ… Correlation heatmap saved")

corr_pearson.to_csv(os.path.join(TABLES_DIR, "correlation_pearson.csv"))
corr_spearman.to_csv(os.path.join(TABLES_DIR, "correlation_spearman.csv"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. VARIANCE INFLATION FACTOR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[C] VIF Analysis...")
# Require at least 2 samples per predictor â€” only feasible if n > n_params
if len(df_stat) > len(STAT_COLS):
    df_vif    = df_stat.dropna()
    X_vif     = sm.add_constant(df_vif)
    vif_data  = pd.DataFrame({
        'Feature': df_vif.columns,
        'VIF'    : [variance_inflation_factor(X_vif.values, i + 1)
                    for i in range(len(df_vif.columns))]
    }).sort_values('VIF', ascending=False)
    print(vif_data.to_string(index=False))
    vif_data.to_csv(os.path.join(TABLES_DIR, "vif.csv"), index=False)
else:
    print(f"  âš ï¸  VIF skipped: n_basins ({len(df_stat)}) â‰¤ n_params ({len(STAT_COLS)})")
    vif_data = pd.DataFrame(columns=['Feature', 'VIF'])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. PCA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[D] Principal Component Analysis...")

# Standardize
scaler    = StandardScaler()
df_scaled = df_stat.fillna(df_stat.median())
X_scaled  = scaler.fit_transform(df_scaled)

pca      = PCA()
scores   = pca.fit_transform(X_scaled)
n_comp   = len(pca.explained_variance_ratio_)

# Scree data
exp_var      = pca.explained_variance_ratio_ * 100
cum_var      = np.cumsum(exp_var)
n_comp_95    = np.searchsorted(cum_var, 95) + 1

print(f"  Total components: {n_comp}")
print(f"  Components to explain 95% variance: {n_comp_95}")
for i in range(min(n_comp, 5)):
    print(f"  PC{i+1}: {exp_var[i]:.2f}%  (cumulative: {cum_var[i]:.2f}%)")

# â”€â”€ Scree plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.bar(range(1, n_comp + 1), exp_var, color='steelblue', alpha=0.8, label='Individual')
ax1.plot(range(1, n_comp + 1), cum_var, 'ro-', ms=5, label='Cumulative')
ax1.axhline(95, color='green', linestyle='--', lw=1.2, label='95% threshold')
ax1.set_xlabel("Principal Component")
ax1.set_ylabel("Explained Variance (%)")
ax1.set_title("Scree Plot â€” PCA")
ax1.legend()
ax1.set_xlim(0.5, n_comp + 0.5)

# â”€â”€ Biplot (PC1 vs PC2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pc1_scores = scores[:, 0]
pc2_scores = scores[:, 1] if n_comp > 1 else np.zeros(len(scores))

ax2.scatter(pc1_scores, pc2_scores, c='darkorange', s=120, zorder=5, edgecolors='black')
for i, bid in enumerate(df_stat.index):
    ax2.annotate(bid, (pc1_scores[i], pc2_scores[i]),
                 textcoords='offset points', xytext=(6, 3), fontsize=9)

# Loading vectors
loadings = pca.components_.T
scale    = max(abs(pc1_scores).max(), abs(pc2_scores).max())
for j, feat in enumerate(STAT_COLS):
    ax2.annotate(
        '', xy=(loadings[j, 0] * scale * 0.5, loadings[j, 1] * scale * 0.5),
        xytext=(0, 0),
        arrowprops=dict(arrowstyle='->', color='royalblue', lw=1.2)
    )
    ax2.text(loadings[j, 0] * scale * 0.55, loadings[j, 1] * scale * 0.55,
             feat, fontsize=6.5, color='royalblue', ha='center')

ax2.set_xlabel(f"PC1 ({exp_var[0]:.1f}%)")
ax2.set_ylabel(f"PC2 ({exp_var[1]:.1f}%)" if n_comp > 1 else "PC2")
ax2.set_title("PCA Biplot (PC1 vs PC2)")
ax2.axhline(0, color='grey', lw=0.5, linestyle='--')
ax2.axvline(0, color='grey', lw=0.5, linestyle='--')

plt.tight_layout()
fig.savefig(os.path.join(PLOTS_DIR, "pca_scree_biplot.png"), dpi=180, bbox_inches='tight')
plt.close(fig)
print("  âœ… PCA scree + biplot saved")

# Save loadings
df_loadings = pd.DataFrame(
    pca.components_[:min(n_comp, 5)].T,
    index=STAT_COLS,
    columns=[f"PC{i+1}" for i in range(min(n_comp, 5))],
)
df_loadings.to_csv(os.path.join(TABLES_DIR, "pca_loadings.csv"))

df_scores_df = pd.DataFrame(
    scores[:, :min(n_comp, 5)],
    index=df_stat.index,
    columns=[f"PC{i+1}" for i in range(min(n_comp, 5))],
)
df_scores_df.to_csv(os.path.join(TABLES_DIR, "pca_scores.csv"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  E. CLUSTER ANALYSIS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[E] Cluster Analysis...")

if len(df_scaled) >= 3:
    # â”€â”€ Hierarchical â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Z = linkage(X_scaled, method='ward')
    fig, ax = plt.subplots(figsize=(10, 5))
    dendrogram(Z, labels=df_stat.index.tolist(), ax=ax, color_threshold=0.7 * max(Z[:, 2]))
    ax.set_title("Hierarchical Clustering Dendrogram (Ward linkage)")
    ax.set_xlabel("Subbasin")
    ax.set_ylabel("Distance")
    plt.tight_layout()
    fig.savefig(os.path.join(PLOTS_DIR, "hierarchical_dendrogram.png"), dpi=180, bbox_inches='tight')
    plt.close(fig)

    # â”€â”€ K-means â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    k_range = range(2, min(len(df_scaled), 4))
    sil_scores = []
    for k in k_range:
        km  = KMeans(n_clusters=k, random_state=42, n_init=10)
        lbs = km.fit_predict(X_scaled)
        if len(set(lbs)) > 1:
            sil_scores.append(silhouette_score(X_scaled, lbs))
        else:
            sil_scores.append(-1)

    best_k = k_range.start + int(np.argmax(sil_scores))
    print(f"  Best k (silhouette): {best_k}")

    km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    CLUSTER_LABELS = km_final.fit_predict(X_scaled)
    df_master['Cluster'] = CLUSTER_LABELS

    # Visualise clusters in PC space
    fig, ax = plt.subplots(figsize=(8, 6))
    scatter = ax.scatter(
        pc1_scores, pc2_scores,
        c=CLUSTER_LABELS, cmap='Set1', s=180, edgecolors='black', zorder=5
    )
    for i, bid in enumerate(df_stat.index):
        ax.annotate(bid, (pc1_scores[i], pc2_scores[i]),
                    textcoords='offset points', xytext=(6, 3), fontsize=9)
    plt.colorbar(scatter, ax=ax, label='Cluster')
    ax.set_xlabel(f"PC1 ({exp_var[0]:.1f}%)")
    ax.set_ylabel(f"PC2 ({exp_var[1]:.1f}%)" if n_comp > 1 else "PC2")
    ax.set_title(f"K-means Clustering (k={best_k}) in PCA Space")
    plt.tight_layout()
    fig.savefig(os.path.join(PLOTS_DIR, "kmeans_clusters.png"), dpi=180, bbox_inches='tight')
    plt.close(fig)
    print(f"  âœ… Cluster analysis complete (k={best_k})")
else:
    print(f"  âš ï¸  Clustering skipped: only {len(df_scaled)} basins (need â‰¥ 3)")
    CLUSTER_LABELS = np.zeros(len(df_stat), dtype=int)
    df_master['Cluster'] = CLUSTER_LABELS

print("\nâœ… SECTION 5 complete.")

"""
=============================================================================
SECTION 6 â€” WATERSHED PRIORITIZATION FRAMEWORK
=============================================================================
Method 1: Compound Parameter Ranking
Method 2: Entropy Weight Method
Method 3: PCA-Based Priority
Kendall's tau comparison + bar charts.
=============================================================================
"""

print("=" * 60)
print("SECTION 6 â€” WATERSHED PRIORITIZATION")
print("=" * 60)

# â”€â”€ Erosion-sensitive parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Direct relation with erosion (higher = more erosion prone â†’ higher rank = worse)
DIRECT_PARAMS = {
    'Drainage_Density_Dd' : 'Dd',
    'Stream_Frequency_Fs' : 'Fs',
    'Rbm'                 : 'Rb',
    'Ruggedness_Rn'       : 'Rn',
    'Relief_Ratio_Rh'     : 'Rh',
    'Hypsometric_HI'      : 'HI',
    'Melton_MRN'          : 'MRN',
}
# Inverse relation (higher = less erosion prone â†’ lower rank = worse)
INVERSE_PARAMS = {
    'Elongation_Ratio_Re' : 'Re',
    'Circularity_Ratio_Rc': 'Rc',
    'Form_Factor_Ff'      : 'Ff',
}

# Keep only params actually in df_master
DIRECT_AVAIL  = {k: v for k, v in DIRECT_PARAMS.items()  if k in df_master.columns}
INVERSE_AVAIL = {k: v for k, v in INVERSE_PARAMS.items() if k in df_master.columns}
ALL_PRIORITY_COLS = list(DIRECT_AVAIL.keys()) + list(INVERSE_AVAIL.keys())

df_pri = df_master[ALL_PRIORITY_COLS].copy().astype(float).fillna(df_master[ALL_PRIORITY_COLS].median())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  METHOD 1 â€” COMPOUND PARAMETER RANKING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[Method 1] Compound Parameter Ranking...")

df_rank = pd.DataFrame(index=df_pri.index)

for col in DIRECT_AVAIL:
    # rank: highest value â†’ rank 1 (most erosion prone)
    df_rank[col] = df_pri[col].rank(ascending=False, method='min')

for col in INVERSE_AVAIL:
    # rank: lowest value â†’ rank 1 (most erosion prone)
    df_rank[col] = df_pri[col].rank(ascending=True, method='min')

df_rank['CF_M1'] = df_rank.mean(axis=1)
df_rank['Rank_M1'] = df_rank['CF_M1'].rank(ascending=True, method='min').astype(int)

# Priority classes
n = len(df_rank)
thresholds = np.percentile(df_rank['CF_M1'], [33, 66])
df_rank['Priority_M1'] = df_rank['CF_M1'].apply(
    lambda x: 'High' if x <= thresholds[0] else ('Moderate' if x <= thresholds[1] else 'Low')
)

print(df_rank[['CF_M1', 'Rank_M1', 'Priority_M1']].to_string())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  METHOD 2 â€” ENTROPY WEIGHT METHOD
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[Method 2] Entropy Weight Method...")

def entropy_weight_score(df, direct_cols, inverse_cols):
    """
    1. Normalise each parameter (0â€“1)
    2. Compute Shannon entropy for each parameter
    3. Derive weights from entropy divergence
    4. Compute weighted score per subbasin
    """
    df_norm = pd.DataFrame(index=df.index)
    for col in direct_cols:
        mn, mx = df[col].min(), df[col].max()
        df_norm[col] = (df[col] - mn) / (mx - mn + 1e-12)  # 0=best 1=worst
    for col in inverse_cols:
        mn, mx = df[col].min(), df[col].max()
        # Invert: low value = high risk â†’ normalise inverted
        df_norm[col] = 1 - (df[col] - mn) / (mx - mn + 1e-12)

    # Entropy for each criterion
    n, m   = df_norm.shape
    weights = []
    for col in df_norm.columns:
        p = df_norm[col] / (df_norm[col].sum() + 1e-12)
        p = p.clip(lower=1e-12)  # avoid log(0)
        e = -np.sum(p * np.log(p)) / np.log(n + 1e-12)
        d = 1 - e
        weights.append(d)

    weights = np.array(weights)
    weights /= (weights.sum() + 1e-12)   # normalise to sum=1

    # Weighted score
    score = (df_norm.values * weights).sum(axis=1)
    return score, dict(zip(df_norm.columns, weights))


score_m2, ew_weights = entropy_weight_score(
    df_pri, list(DIRECT_AVAIL.keys()), list(INVERSE_AVAIL.keys())
)
df_rank['Score_M2'] = score_m2
df_rank['Rank_M2']  = pd.Series(score_m2, index=df_pri.index).rank(
    ascending=False, method='min'
).astype(int)

thresh_m2 = np.percentile(score_m2, [66, 33])
df_rank['Priority_M2'] = df_rank['Score_M2'].apply(
    lambda x: 'High' if x >= thresh_m2[0] else ('Moderate' if x >= thresh_m2[1] else 'Low')
)

print("  Entropy weights:")
for k, w in sorted(ew_weights.items(), key=lambda x: -x[1]):
    print(f"    {k}: {w:.4f}")
print(df_rank[['Score_M2', 'Rank_M2', 'Priority_M2']].to_string())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  METHOD 3 â€” PCA-BASED PRIORITY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[Method 3] PCA-Based Priority...")

# Re-run PCA on priority parameters only
scaler_p   = StandardScaler()
X_p        = scaler_p.fit_transform(df_pri.fillna(df_pri.median()))
pca_p      = PCA()
scores_p   = pca_p.fit_transform(X_p)
exp_var_p  = pca_p.explained_variance_ratio_

# Composite score: weighted sum of PC scores by explained variance
n_retain = min(3, len(exp_var_p))
weights_p = exp_var_p[:n_retain] / exp_var_p[:n_retain].sum()

# Sign convention: check if PC1 aligns with erosion risk
# (higher PC1 loading on Dd/Rn = higher risk = positive score)
pc1_loadings = pd.Series(pca_p.components_[0], index=ALL_PRIORITY_COLS)
direct_sign  = np.sign(pc1_loadings[list(DIRECT_AVAIL.keys())].mean())
if direct_sign < 0:
    scores_p = -scores_p   # flip sign

pca_composite = (scores_p[:, :n_retain] * weights_p).sum(axis=1)
df_rank['Score_M3'] = pca_composite
df_rank['Rank_M3']  = pd.Series(pca_composite, index=df_pri.index).rank(
    ascending=False, method='min'
).astype(int)

thresh_m3 = np.percentile(pca_composite, [66, 33])
df_rank['Priority_M3'] = df_rank['Score_M3'].apply(
    lambda x: 'High' if x >= thresh_m3[0] else ('Moderate' if x >= thresh_m3[1] else 'Low')
)
print(df_rank[['Score_M3', 'Rank_M3', 'Priority_M3']].to_string())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  COMPARISON â€” KENDALL's TAU
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[Comparison] Kendall's tau agreement analysis...")

r12, p12 = stats.kendalltau(df_rank['Rank_M1'], df_rank['Rank_M2'])
r13, p13 = stats.kendalltau(df_rank['Rank_M1'], df_rank['Rank_M3'])
r23, p23 = stats.kendalltau(df_rank['Rank_M2'], df_rank['Rank_M3'])

df_kendall = pd.DataFrame({
    'Comparison': ['M1 vs M2', 'M1 vs M3', 'M2 vs M3'],
    'Kendall_tau': [r12, r13, r23],
    'p_value'    : [p12, p13, p23],
    'Agreement'  : ['Strong' if abs(r) > 0.7 else 'Moderate' if abs(r) > 0.4 else 'Weak'
                    for r in [r12, r13, r23]],
})
print(df_kendall.to_string(index=False))

# â”€â”€ Ranking comparison bar chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

basins  = df_rank.index.tolist()
x       = np.arange(len(basins))
width   = 0.28

ax1.bar(x - width, df_rank['Rank_M1'], width, label='Method 1 (Compound)',  color='steelblue')
ax1.bar(x,         df_rank['Rank_M2'], width, label='Method 2 (Entropy)',   color='darkorange')
ax1.bar(x + width, df_rank['Rank_M3'], width, label='Method 3 (PCA-based)', color='green')
ax1.set_xticks(x)
ax1.set_xticklabels(basins)
ax1.set_ylabel("Rank (1 = Highest Priority)")
ax1.set_title("Prioritization Rank Comparison Across Methods")
ax1.legend()
ax1.invert_yaxis()   # rank 1 at top

# Priority colour map
priority_map = {'High': '#d73027', 'Moderate': '#fee090', 'Low': '#4575b4'}
for i, bid in enumerate(basins):
    for j, (col, method) in enumerate([
        ('Priority_M1', 'M1'), ('Priority_M2', 'M2'), ('Priority_M3', 'M3')
    ]):
        ax2.bar(i * 4 + j, 1,
                color=priority_map.get(df_rank.loc[bid, col], 'grey'),
                edgecolor='black', linewidth=0.7)
        ax2.text(i * 4 + j, 0.5, df_rank.loc[bid, col][:1],
                 ha='center', va='center', fontsize=9, fontweight='bold')

ax2.set_xticks([i * 4 + 1 for i in range(len(basins))])
ax2.set_xticklabels(basins)
ax2.set_title("Priority Class by Method")
legend_patches = [mpatches.Patch(color=v, label=k) for k, v in priority_map.items()]
ax2.legend(handles=legend_patches, loc='upper right')
ax2.set_yticks([])

plt.tight_layout()
fig.savefig(os.path.join(PLOTS_DIR, "prioritization_comparison.png"), dpi=180, bbox_inches='tight')
plt.close(fig)

# â”€â”€ Save outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ranking_table = df_rank[['CF_M1','Rank_M1','Priority_M1',
                          'Score_M2','Rank_M2','Priority_M2',
                          'Score_M3','Rank_M3','Priority_M3']].copy()
ranking_table.to_csv(os.path.join(TABLES_DIR, "prioritization_ranking.csv"))
df_kendall.to_csv(os.path.join(TABLES_DIR, "kendall_tau.csv"), index=False)

# Save priority shapefile
gdf_priority = gdf_sub.merge(
    ranking_table.reset_index(), on='basin_id', how='left'
)
gdf_priority.to_file(os.path.join(SHAPES_DIR, "subbasins_priority.shp"))

print(f"\n  âœ… Priority shapefile saved: {SHAPES_DIR}subbasins_priority.shp")
print("\nâœ… SECTION 6 complete.")
print("\n  FINAL RANKING TABLE:")
print(ranking_table.to_string())

"""
=============================================================================
SECTION 7 â€” ADVANCED PLOTLY INTERACTIVE VISUALIZATIONS
=============================================================================
All figures saved as interactive HTML + static PNG.
=============================================================================
"""

print("=" * 60)
print("SECTION 7 â€” PLOTLY INTERACTIVE VISUALIZATION SUITE")
print("=" * 60)

HTML_DIR = os.path.join(PLOTS_DIR, "html/")
os.makedirs(HTML_DIR, exist_ok=True)


def save_fig(fig, name):
    """Save Plotly figure as HTML and static PNG."""
    html_path = os.path.join(HTML_DIR, f"{name}.html")
    fig.write_html(html_path, include_plotlyjs='cdn')
    print(f"  âœ… {name}.html")
    return html_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  1. HORTON'S LAWS â€” Stream Number & Stream Length
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[1] Horton's Law plots...")

for bid, df_lin in LINEAR_PER_ORDER.items():
    if df_lin.empty or len(df_lin) < 2:
        continue
    orders   = df_lin['order'].values
    Nu_vals  = df_lin['Nu'].values.astype(float)
    Lu_vals  = (df_lin['Lu'].values / 1000).astype(float)  # km

    # Regression on log scale (exclude zeros)
    mask_n = Nu_vals > 0
    log_Nu = np.log10(Nu_vals[mask_n])
    log_u  = np.log10(orders[mask_n])
    if len(log_u) > 1:
        slope_n, intercept_n, r_n, p_n, _ = stats.linregress(log_u, log_Nu)
        r2_n = r_n ** 2
    else:
        slope_n, intercept_n, r2_n = 0, 0, 0

    mask_l = Lu_vals > 0
    log_Lu = np.log10(Lu_vals[mask_l])
    if len(log_u[mask_l[:len(log_u)]]) > 1:
        slope_l, intercept_l, r_l, _, _ = stats.linregress(
            log_u[:len(log_Lu)], log_Lu
        )
        r2_l = r_l ** 2
    else:
        slope_l, intercept_l, r2_l = 0, 0, 0

    fig = make_subplots(rows=1, cols=2,
                        subplot_titles=[
                            f"Stream Number Law â€” {bid}",
                            f"Stream Length Law â€” {bid}"
                        ])

    # Stream number
    fig.add_trace(go.Scatter(
        x=orders[mask_n], y=Nu_vals[mask_n], mode='markers+lines',
        name='Stream Number', marker=dict(size=10, color='royalblue'),
        hovertemplate='Order %{x}: %{y} streams',
    ), row=1, col=1)
    fit_x = np.linspace(orders.min(), orders.max(), 50)
    fit_y = 10 ** (intercept_n + slope_n * np.log10(fit_x))
    fig.add_trace(go.Scatter(
        x=fit_x, y=fit_y, mode='lines',
        name=f'Regression (RÂ²={r2_n:.3f})',
        line=dict(color='firebrick', dash='dash'),
    ), row=1, col=1)

    # Stream length
    fig.add_trace(go.Scatter(
        x=orders[mask_l], y=Lu_vals[mask_l], mode='markers+lines',
        name='Stream Length (km)', marker=dict(size=10, color='darkorange'),
        hovertemplate='Order %{x}: %{y:.2f} km',
    ), row=1, col=2)
    if r2_l > 0:
        fit_yl = 10 ** (intercept_l + slope_l * np.log10(fit_x))
        fig.add_trace(go.Scatter(
            x=fit_x, y=fit_yl, mode='lines',
            name=f'Regression (RÂ²={r2_l:.3f})',
            line=dict(color='green', dash='dash'),
        ), row=1, col=2)

    fig.update_xaxes(type='log', title_text='Stream Order (log)', row=1, col=1)
    fig.update_yaxes(type='log', title_text='Stream Number (log)', row=1, col=1)
    fig.update_xaxes(type='log', title_text='Stream Order (log)', row=1, col=2)
    fig.update_yaxes(type='log', title_text='Stream Length km (log)', row=1, col=2)
    fig.update_layout(title=f"Horton's Laws â€” {bid}", template='plotly_white',
                      height=500, showlegend=True)
    save_fig(fig, f"01_hortons_law_{bid}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  2. RADAR CHART â€” Morphometric Signature per Subbasin
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[2] Radar charts...")

radar_params = [
    'Drainage_Density_Dd', 'Stream_Frequency_Fs', 'Form_Factor_Ff',
    'Elongation_Ratio_Re', 'Circularity_Ratio_Rc', 'Ruggedness_Rn',
    'Hypsometric_HI', 'Relief_Ratio_Rh', 'Rbm',
]
radar_params = [p for p in radar_params if p in df_master.columns]

df_radar = df_master[radar_params].copy().astype(float)
# Normalise 0-1 for radar
df_radar_norm = (df_radar - df_radar.min()) / (df_radar.max() - df_radar.min() + 1e-12)

fig = go.Figure()
categories = [p.split('_')[-1] for p in radar_params]
colors_r   = px.colors.qualitative.Set2

for i, (bid, row) in enumerate(df_radar_norm.iterrows()):
    vals = row.tolist()
    vals += [vals[0]]  # close polygon
    fig.add_trace(go.Scatterpolar(
        r=vals, theta=categories + [categories[0]],
        fill='toself', name=bid,
        line_color=colors_r[i % len(colors_r)],
        opacity=0.6,
        hovertemplate=bid + '<br>%{theta}: %{r:.3f}',
    ))

fig.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
    title="Morphometric Signature Radar Chart â€” All Subbasins",
    template='plotly_white', height=600,
)
save_fig(fig, "02_radar_morphometric")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  3. SCATTER MATRIX
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[3] Scatter matrix...")
scatter_cols = [c for c in ['Drainage_Density_Dd', 'Stream_Frequency_Fs',
                              'Elongation_Ratio_Re', 'Basin_Relief_H_m',
                              'Ruggedness_Rn', 'Hypsometric_HI']
                if c in df_master.columns]
df_sc = df_master[scatter_cols].reset_index()
fig   = px.scatter_matrix(
    df_sc, dimensions=scatter_cols, color='basin_id',
    title="Scatter Matrix â€” Key Morphometric Parameters",
    labels={c: c.split('_')[-1] for c in scatter_cols},
    template='plotly_white',
)
fig.update_traces(diagonal_visible=False, showupperhalf=False)
save_fig(fig, "03_scatter_matrix")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  4. 3D SCATTER â€” Dd vs Relief vs Area
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[4] 3D scatter...")
if all(c in df_master.columns for c in ['Drainage_Density_Dd', 'Basin_Relief_H_m', 'Area_km2']):
    df3d = df_master[['Drainage_Density_Dd', 'Basin_Relief_H_m', 'Area_km2']].reset_index()
    fig  = px.scatter_3d(
        df3d, x='Drainage_Density_Dd', y='Basin_Relief_H_m', z='Area_km2',
        color='basin_id', text='basin_id',
        title="3D Scatter: Drainage Density vs Relief vs Area",
        labels={'Drainage_Density_Dd': 'Dd (km/kmÂ²)',
                'Basin_Relief_H_m': 'Relief (m)',
                'Area_km2': 'Area (kmÂ²)'},
        template='plotly_white', size_max=18,
    )
    save_fig(fig, "04_3d_scatter")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  5. HISTOGRAM DISTRIBUTIONS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[5] Histogram distributions...")
hist_cols = [c for c in STAT_COLS if c in df_master.columns][:9]
fig = make_subplots(rows=3, cols=3, subplot_titles=hist_cols)
for i, col in enumerate(hist_cols):
    r, c_idx = divmod(i, 3)
    fig.add_trace(
        go.Histogram(x=df_master[col].dropna(), name=col,
                     marker_color=px.colors.qualitative.Set1[i % 9],
                     nbinsx=10),
        row=r+1, col=c_idx+1,
    )
fig.update_layout(title="Parameter Distributions", template='plotly_white',
                  height=800, showlegend=False)
save_fig(fig, "05_histograms")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  6. BOX PLOTS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[6] Box plots...")
df_melt = df_master[STAT_COLS[:10]].reset_index().melt(id_vars='basin_id')
fig     = px.box(
    df_melt, x='variable', y='value', color='variable',
    title="Box Plot â€” Morphometric Parameters",
    template='plotly_white', points='all',
    labels={'variable': 'Parameter', 'value': 'Value'},
)
fig.update_xaxes(tickangle=45)
save_fig(fig, "06_boxplots")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  7. HYPSOMETRIC CURVES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[7] Hypsometric curves...")
if HYPS:
    fig = go.Figure()
    colors_h = px.colors.qualitative.Plotly
    for i, (bid, (rel_area, rel_elev)) in enumerate(HYPS.items()):
        hi_val = df_master.loc[bid, 'Hypsometric_HI'] if 'Hypsometric_HI' in df_master.columns else np.nan
        fig.add_trace(go.Scatter(
            x=rel_area, y=rel_elev, mode='lines',
            name=f"{bid} (HI={hi_val:.3f})" if not np.isnan(hi_val) else bid,
            line=dict(color=colors_h[i % len(colors_h)], width=2),
            hovertemplate='Rel. Area: %{x:.2f}<br>Rel. Elev: %{y:.2f}',
        ))
    # Reference lines
    fig.add_trace(go.Scatter(
        x=[0, 1], y=[0.5, 0.5], mode='lines',
        name='HI = 0.5 (Equilibrium)', line=dict(dash='dash', color='grey'),
    ))
    fig.update_layout(
        title="Hypsometric Curves â€” All Subbasins",
        xaxis_title="Relative Area (a/A)",
        yaxis_title="Relative Elevation (h/H)",
        template='plotly_white', height=550,
    )
    save_fig(fig, "07_hypsometric_curves")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  8. PLOTLY CORRELATION HEATMAP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[8] Plotly correlation heatmap...")
fig = go.Figure(go.Heatmap(
    z=corr_pearson.values,
    x=corr_pearson.columns.tolist(),
    y=corr_pearson.index.tolist(),
    colorscale='RdYlBu', zmid=0, zmin=-1, zmax=1,
    text=np.round(corr_pearson.values, 2).astype(str),
    texttemplate='%{text}', textfont_size=8,
    hovertemplate='%{y} vs %{x}: %{z:.3f}',
))
fig.update_layout(title="Pearson Correlation Matrix (Interactive)",
                  template='plotly_white', height=700, width=800)
save_fig(fig, "08_correlation_heatmap")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  9. PARALLEL COORDINATE PLOT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[9] Parallel coordinate plot...")
par_cols = [c for c in STAT_COLS if c in df_master.columns][:8]
df_par   = df_master[par_cols].reset_index()
df_par['basin_num'] = range(len(df_par))
fig = px.parallel_coordinates(
    df_par, color='basin_num', dimensions=par_cols,
    color_continuous_scale=px.colors.diverging.Tealrose,
    title="Parallel Coordinate Plot â€” Morphometric Parameters",
    labels={c: c.replace('_', ' ') for c in par_cols},
)
save_fig(fig, "09_parallel_coordinates")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  10. BUBBLE PLOT â€” Area vs Dd sized by Relief
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[10] Bubble plot...")
if all(c in df_master.columns for c in ['Area_km2', 'Drainage_Density_Dd', 'Basin_Relief_H_m']):
    df_bub = df_master[['Area_km2', 'Drainage_Density_Dd', 'Basin_Relief_H_m']].reset_index()
    fig    = px.scatter(
        df_bub, x='Area_km2', y='Drainage_Density_Dd',
        size='Basin_Relief_H_m', color='basin_id', text='basin_id',
        title="Area vs Drainage Density (size = Basin Relief)",
        labels={'Area_km2': 'Area (kmÂ²)',
                'Drainage_Density_Dd': 'Drainage Density (km/kmÂ²)',
                'Basin_Relief_H_m': 'Relief (m)'},
        template='plotly_white', size_max=50,
    )
    save_fig(fig, "10_bubble_area_dd_relief")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  11. PRIORITY MAP â€” Interactive Plotly choropleth-style
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[11] Priority class map...")
priority_color = {'High': '#d73027', 'Moderate': '#fee090', 'Low': '#4575b4'}
fig = go.Figure()

for _, row in gdf_priority.iterrows():
    bid  = row['basin_id']
    pri  = row.get('Priority_M1', 'Unknown')
    col  = priority_color.get(pri, 'grey')
    geom = row.geometry

    if geom.geom_type == 'Polygon':
        geoms = [geom]
    else:
        geoms = list(geom.geoms)

    for g in geoms:
        coords = np.array(g.exterior.coords)
        fig.add_trace(go.Scatter(
            x=coords[:, 0], y=coords[:, 1],
            fill='toself', fillcolor=col,
            line=dict(color='black', width=1.5),
            name=f"{bid} ({pri})",
            opacity=0.75,
            hovertemplate=(
                f"<b>{bid}</b><br>"
                f"Priority: {pri}<br>"
                f"Rank M1: {row.get('Rank_M1','â€”')}<br>"
                f"Rank M2: {row.get('Rank_M2','â€”')}<br>"
                f"Rank M3: {row.get('Rank_M3','â€”')}<br>"
                f"Dd: {row.get('Drainage_Density_Dd','â€”')}"
            ),
        ))

fig.update_layout(
    title="Watershed Priority Classification Map (Method 1 â€” Compound Ranking)",
    xaxis=dict(title="Easting (m)", scaleanchor='y'),
    yaxis=dict(title="Northing (m)"),
    template='plotly_white', height=650,
    showlegend=True,
)
save_fig(fig, "11_priority_map")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  12. ELEVATION PROFILE (Main Channel)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[12] Elevation profiles...")

def extract_stream_profile(stream_gdf, dem_path, basin_id, n_points=200):
    """Sample DEM along the longest stream segment in a basin."""
    segs = stream_gdf[stream_gdf.get('basin_id', stream_gdf.index) == basin_id]
    if len(segs) == 0:
        return None, None, None
    longest_seg = segs.loc[segs.geometry.length.idxmax()]
    geom = longest_seg.geometry

    if geom.geom_type == 'MultiLineString':
        geom = linemerge(geom)
    if geom.geom_type != 'LineString':
        return None, None, None

    distances = np.linspace(0, geom.length, n_points)
    pts       = [geom.interpolate(d) for d in distances]

    with rasterio.open(dem_path) as src:
        elevations = []
        for pt in pts:
            r_idx, c_idx = rowcol(src.transform, pt.x, pt.y)
            try:
                elev = src.read(1)[r_idx, c_idx]
                nodata = src.nodata if src.nodata else -9999
                elevations.append(np.nan if elev == nodata else float(elev))
            except IndexError:
                elevations.append(np.nan)

    return np.array(distances / 1000), np.array(elevations), geom


fig_profiles = make_subplots(
    rows=len(gdf_sub), cols=1,
    shared_xaxes=False,
    subplot_titles=[f"Longitudinal Profile â€” {bid}" for bid in gdf_sub['basin_id']],
    vertical_spacing=0.08,
)

for i, (_, row) in enumerate(gdf_sub.iterrows()):
    bid = row['basin_id']
    # Assign basin_id to stream order dataframe if not present
    if 'basin_id' not in gdf_so_sub.columns:
        break
    dist, elev, _ = extract_stream_profile(gdf_so_sub, RASTERS['dem'], bid)
    if dist is None:
        continue
    valid = ~np.isnan(elev)
    fig_profiles.add_trace(
        go.Scatter(
            x=dist[valid], y=elev[valid],
            mode='lines', name=bid, fill='tozeroy',
            line=dict(color=px.colors.qualitative.Plotly[i % 10], width=2),
            hovertemplate='Distance: %{x:.2f} km<br>Elevation: %{y:.1f} m',
        ),
        row=i+1, col=1,
    )
    fig_profiles.update_xaxes(title_text="Distance from outlet (km)", row=i+1, col=1)
    fig_profiles.update_yaxes(title_text="Elevation (m)", row=i+1, col=1)

fig_profiles.update_layout(
    title="Longitudinal Stream Profiles â€” All Subbasins",
    template='plotly_white', height=300 * len(gdf_sub), showlegend=True,
)
save_fig(fig_profiles, "12_longitudinal_profiles")

print(f"\nâœ… SECTION 7 complete. HTML files in: {HTML_DIR}")
print(f"   Total figures: 12")

print("=" * 60)
print("SECTION 8 â€” OUTPUT EXPORT")
print("=" * 60)

# â”€â”€ 1. Master morphometric table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
master_csv = os.path.join(TABLES_DIR, "morphometric_master_table.csv")
df_master.to_csv(master_csv)
print(f"\n[1] Master table â†’ {master_csv}")

print("\n  â”€â”€ First 10 rows (all basins if â‰¤ 10) â”€â”€")
print(df_master.head(10).to_string())

# â”€â”€ 2. Stream order summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[2] Stream Order Summary:")
all_order_rows = []
for bid, df_lin in LINEAR_PER_ORDER.items():
    df_lin_c = df_lin.copy()
    df_lin_c['basin_id'] = bid
    all_order_rows.append(df_lin_c)

if all_order_rows:
    df_order_summary = pd.concat(all_order_rows, ignore_index=True)
    df_order_summary.to_csv(os.path.join(TABLES_DIR, "stream_order_summary.csv"), index=False)
    print(df_order_summary.to_string(index=False))

# â”€â”€ 3. Statistical summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[3] Statistical Summary (mean Â± std):")
for col in STAT_COLS[:12]:
    if col in df_master.columns:
        mn  = df_master[col].mean()
        sd  = df_master[col].std()
        cv  = (sd / mn * 100) if mn != 0 else np.nan
        print(f"  {col:<35s} {mn:>10.4f} Â± {sd:>8.4f}  (CV={cv:>6.1f}%)")

# â”€â”€ 4. Ranking table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[4] Prioritization Ranking:")
rank_csv = os.path.join(TABLES_DIR, "prioritization_ranking.csv")
print(pd.read_csv(rank_csv, index_col=0).to_string())

# â”€â”€ 5. Priority classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n[5] Priority Classification Summary:")
for bid in gdf_sub['basin_id']:
    if bid in df_rank.index:
        r = df_rank.loc[bid]
        print(f"  {bid}: M1={r['Priority_M1']:<8} M2={r['Priority_M2']:<8} M3={r['Priority_M3']}")

# â”€â”€ 6. Summary of all output files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\n[6] Output files:")
for root, dirs, files in os.walk(OUT_DIR):
    for f in sorted(files):
        full = os.path.join(root, f)
        size = os.path.getsize(full) / 1024
        print(f"  {full.replace(OUT_DIR, ''):<60s}  {size:>8.1f} KB")

print("\nâœ… SECTION 8 complete.")


"""
=============================================================================
SECTION 9 â€” AUTOMATED REPORT GENERATION
=============================================================================
Generates a structured text report suitable for publication drafting.
=============================================================================
"""

print("\n" + "=" * 60)
print("SECTION 9 â€” REPORT GENERATION")
print("=" * 60)

def format_val(val, decimals=3):
    """Format float or return 'N/A'."""
    try:
        return f"{float(val):.{decimals}f}"
    except (TypeError, ValueError):
        return "N/A"


def build_report():
    basin_ids = gdf_sub['basin_id'].tolist()
    n_basins  = len(basin_ids)

    # Study area bounding box in geographic coords
    bounds = gdf_sub.to_crs("EPSG:4326").total_bounds
    lon_min, lat_min, lon_max, lat_max = bounds

    total_area = df_master['Area_km2'].sum()
    elev_min   = df_master['Elev_Min_m'].min()
    elev_max   = df_master['Elev_Max_m'].max()
    mean_dd    = df_master['Drainage_Density_Dd'].mean()
    mean_re    = df_master['Elongation_Ratio_Re'].mean()
    mean_hi    = df_master['Hypsometric_HI'].mean() if 'Hypsometric_HI' in df_master.columns else np.nan
    mean_rn    = df_master['Ruggedness_Rn'].mean() if 'Ruggedness_Rn' in df_master.columns else np.nan

    high_pri   = df_rank[df_rank['Priority_M1'] == 'High'].index.tolist()
    mod_pri    = df_rank[df_rank['Priority_M1'] == 'Moderate'].index.tolist()
    low_pri    = df_rank[df_rank['Priority_M1'] == 'Low'].index.tolist()

    lines = []
    def s(text=""):
        lines.append(text)

    s("=" * 80)
    s("MORPHOMETRIC ANALYSIS OF A WATERSHED")
    s("Generated by Automated Morphometric Analysis Tool")
    s("Based on: Horton (1945), Strahler (1952, 1964), Schumm (1956), Miller (1953)")
    s("=" * 80)

    s()
    s("1. STUDY AREA DESCRIPTION")
    s("-" * 40)
    s(f"The study area comprises {n_basins} subbasins covering a total area of "
      f"{total_area:.2f} kmÂ². The watershed extends from approximately "
      f"{lat_min:.4f}Â°N to {lat_max:.4f}Â°N and {lon_min:.4f}Â°E to {lon_max:.4f}Â°E. "
      f"Elevation ranges from {elev_min:.0f} m to {elev_max:.0f} m above sea level, "
      f"indicating a relief of {elev_max - elev_min:.0f} m across the watershed. "
      f"The analysis utilises SRTM 30 m Digital Elevation Model data processed "
      f"in a UTM projected coordinate reference system ({UTM_EPSG}) to ensure "
      f"accurate area and length computations.")

    s()
    s("2. DATA SOURCES")
    s("-" * 40)
    s("â€¢ DEM: Shuttle Radar Topography Mission (SRTM) 30 m resolution (NASA/USGS)")
    s("â€¢ Subbasins: Derived from DEM hydrological processing (see shapefile for polygon count)")
    s("â€¢ Stream network: Extracted via D8 flow routing with Strahler ordering")
    s("â€¢ Flow direction: D8 algorithm (ArcGIS/QGIS/TauDEM compatible)")
    s("â€¢ Flow accumulation: Derived from D8 flow direction")
    s("â€¢ CRS: Reprojected to UTM for accurate metric computations")

    s()
    s("3. METHODOLOGY")
    s("-" * 40)
    s("Morphometric analysis was performed following the methodologies of "
      "Horton (1945) for stream ordering and bifurcation laws, Strahler (1952, 1964) "
      "for the hierarchical stream order classification, Schumm (1956) for basin "
      "geometry and elongation ratio, and Miller (1953) for circularity ratio. "
      "Linear, areal, and relief morphometric parameters were computed at the "
      "subbasin level using SRTM DEM data and derived GIS layers. "
      "Watershed prioritization employed three independent methods: Compound "
      "Parameter Ranking, Entropy Weight Method, and PCA-Based Priority scoring, "
      "with inter-method agreement assessed using Kendall's tau.")

    s()
    s("4. MORPHOMETRIC RESULTS")
    s("-" * 40)
    s()
    s("4.1 Linear Aspects")
    for bid in basin_ids:
        if bid not in LINEAR_PER_ORDER:
            continue
        df_lin = LINEAR_PER_ORDER[bid]
        max_ord = df_lin['order'].max()
        Rbm_v   = df_linear_summary.loc[bid, 'Rbm'] if bid in df_linear_summary.index else np.nan
        tot_N   = df_lin['Nu'].sum()
        s(f"  {bid}: {int(max_ord)}-order basin, {int(tot_N)} stream segments, "
          f"Mean Bifurcation Ratio (Rbm) = {format_val(Rbm_v)}. "
          f"{'Rbm values between 3â€“5 indicate normal basins without structural disturbances.' if 3 <= Rbm_v <= 5 else 'Rbm outside 3â€“5 range may indicate structural control.'}")

    s()
    s("4.2 Areal Aspects")
    for bid in basin_ids:
        if bid not in df_master.index:
            continue
        row = df_master.loc[bid]
        s(f"  {bid}: Area={format_val(row.get('Area_km2'))} kmÂ², "
          f"Dd={format_val(row.get('Drainage_Density_Dd'))} km/kmÂ², "
          f"Re={format_val(row.get('Elongation_Ratio_Re'))}, "
          f"Rc={format_val(row.get('Circularity_Ratio_Rc'))}, "
          f"Ff={format_val(row.get('Form_Factor_Ff'))}, "
          f"Shape: {row.get('Shape_Class','â€”')}.")

    s()
    s("4.3 Relief Aspects")
    for bid in basin_ids:
        if bid not in df_master.index:
            continue
        row = df_master.loc[bid]
        hi_interp = row.get('Hyps_Class', 'â€”')
        s(f"  {bid}: H={format_val(row.get('Basin_Relief_H_m'),0)} m, "
          f"Rh={format_val(row.get('Relief_Ratio_Rh'),5)}, "
          f"Rn={format_val(row.get('Ruggedness_Rn'))}, "
          f"HI={format_val(row.get('Hypsometric_HI'))}, "
          f"Stage: {hi_interp}.")

    s()
    s("5. STATISTICAL ANALYSIS")
    s("-" * 40)
    s(f"Mean drainage density across all subbasins: {format_val(mean_dd)} km/kmÂ². "
      f"{'High drainage density (>3.5 km/kmÂ²) implies impermeable lithology, steep slopes, and sparse vegetation, leading to rapid surface runoff.' if mean_dd > 3.5 else 'Moderate to low drainage density suggests permeable materials and gentle topography.'}"
      " PCA revealed that the first two principal components explained the majority "
      f"of total variance ({exp_var[0]:.1f}% + {exp_var[1]:.1f}% = "
      f"{exp_var[0]+exp_var[1]:.1f}%), with drainage density and relief parameters "
      "dominating PC1 and basin shape parameters dominating PC2.")

    s()
    s("6. WATERSHED PRIORITIZATION")
    s("-" * 40)
    s(f"Three independent methods identified the following priority classes:")
    s(f"  HIGH priority basins (most susceptible to erosion): {', '.join(high_pri) if high_pri else 'None'}")
    s(f"  MODERATE priority basins: {', '.join(mod_pri) if mod_pri else 'None'}")
    s(f"  LOW priority basins: {', '.join(low_pri) if low_pri else 'None'}")
    s(f"Inter-method agreement (Kendall's Ï„): M1 vs M2 = {format_val(r12,3)}, "
      f"M1 vs M3 = {format_val(r13,3)}, M2 vs M3 = {format_val(r23,3)}. "
      f"{'High agreement across methods validates the prioritization framework.' if min(abs(r12),abs(r13),abs(r23))>0.5 else 'Moderate agreement suggests parameter-sensitivity in ranking.'}")

    s()
    s("7. DISCUSSION")
    s("-" * 40)
    s("Drainage Density Implications:")
    s(f"  The watershed exhibits a mean Dd of {format_val(mean_dd)} km/kmÂ². "
      "High Dd values indicate fine texture, less permeable lithology, and "
      "greater surface runoff propensity (Horton, 1945). Basins with Dd > 3.5 "
      "are expected to respond rapidly to rainfall events, increasing flood risk.")
    s()
    s("Shape and Runoff Response:")
    s(f"  Mean Elongation Ratio (Re) = {format_val(mean_re)}. "
      f"{'Elongated basins (Re < 0.6) have lower peak discharge and extended concentration time.' if mean_re < 0.6 else 'Sub-circular to circular basins (Re > 0.7) generate higher and faster flood peaks.'} "
      "Form factor and circularity ratio confirm this assessment.")
    s()
    s("Relief and Erosion:")
    s(f"  Mean Ruggedness Number (Rn) = {format_val(mean_rn)}. "
      "High Rn reflects steep slopes combined with high drainage density, "
      "indicating high erosion potential and flash flood susceptibility "
      "(Strahler, 1964).")
    s()
    s("Hypsometric Stage:")
    s(f"  Mean Hypsometric Integral (HI) = {format_val(mean_hi)}. "
      f"{'HI > 0.6 indicates monadnock/young stage â€” active erosion, convex slopes.' if mean_hi > 0.6 else 'HI 0.35â€“0.6 indicates mature equilibrium stage.' if mean_hi > 0.35 else 'HI < 0.35 indicates peneplain/old stage â€” reduced erosion activity.'}")

    s()
    s("8. CONCLUSION")
    s("-" * 40)
    s(f"This study presents a comprehensive morphometric analysis of a {n_basins}-subbasin "
      f"watershed using SRTM 30 m DEM. The integrated analysis of linear, areal, and "
      f"relief parameters reveals the geomorphic maturity, erosion susceptibility, "
      f"and hydrological response characteristics of each subbasin. "
      f"Subbasins {', '.join(high_pri)} are identified as highest priority for "
      f"soil and water conservation interventions based on convergent evidence "
      f"from three independent prioritization methods. Findings are reproducible "
      f"and suitable for integration into watershed management planning frameworks.")

    s()
    s("9. REFERENCES")
    s("-" * 40)
    s("Horton, R.E. (1945). Erosional development of streams and their drainage basins.")
    s("  Geological Society of America Bulletin, 56(3), 275â€“370.")
    s()
    s("Miller, V.C. (1953). A quantitative geomorphic study of drainage basin characteristics")
    s("  in the Clinch Mountain area, Virginia and Tennessee. Columbia University, Tech. Rep.")
    s()
    s("Schumm, S.A. (1956). Evolution of drainage systems and slopes in badlands at Perth")
    s("  Amboy, New Jersey. Geological Society of America Bulletin, 67(5), 597â€“646.")
    s()
    s("Strahler, A.N. (1952). Hypsometric (area-altitude) analysis of erosional topography.")
    s("  Geological Society of America Bulletin, 63(11), 1117â€“1142.")
    s()
    s("Strahler, A.N. (1964). Quantitative geomorphology of drainage basins and channel")
    s("  networks. In Handbook of Applied Hydrology (ed. V.T. Chow), pp. 4.39â€“4.76.")
    s()
    s("Hack, J.T. (1957). Studies of longitudinal stream profiles in Virginia and Maryland.")
    s("  USGS Professional Paper 294-B.")
    s()
    s("Melton, M.A. (1965). The geomorphic and palaeoclimatic significance of alluvial")
    s("  deposits in Southern Arizona. Journal of Geology, 73(1), 1â€“38.")
    s()
    s("Riley, S.J., DeGloria, S.D., Elliot, R. (1999). A terrain ruggedness index that")
    s("  quantifies topographic heterogeneity. Intermountain Journal of Sciences, 5, 23â€“27.")

    s()
    s("=" * 80)
    s("END OF REPORT")
    s("=" * 80)

    return "\n".join(lines)


report_text = build_report()
report_path = os.path.join(REPORT_DIR, "morphometric_analysis_report.txt")
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(report_text)

print("\nREPORT PREVIEW (first 40 lines):")
print("â€”" * 60)
for line in report_text.split("\n")[:40]:
    print(line)
print("...")
print("â€”" * 60)
print(f"\nâœ… Full report saved: {report_path}")
print("\nâœ… SECTION 9 complete.")

print("\n" + "=" * 60)
print("  ğŸ‰  ALL SECTIONS COMPLETE")
print("=" * 60)
print(f"  Output root: {OUT_DIR}")
print(f"  Maps (9)   : {MAPS_DIR}")
print(f"  Plots HTML : {HTML_DIR}")
print(f"  Tables     : {TABLES_DIR}")
print(f"  Shapefiles : {SHAPES_DIR}")
print(f"  Report     : {REPORT_DIR}")

print("\n[F] Generating Tectonic Activity Map...")

iat_color_map = {
    'Class 1 â€” Very High': '#d73027',
    'Class 2 â€” High'     : '#fc8d59',
    'Class 3 â€” Moderate' : '#fee08b',
    'Class 4 â€” Low'      : '#91bfdb',
}

gdf_iat = gdf_sub.merge(df_IAT[['IAT','IAT_class']].reset_index(), on='basin_id')

utm_ext  = compute_utm_extent()
fig, ax, utm_ext  = base_axes("Index of Active Tectonics (IAT) â€” El Hamdouni et al., 2008")

for _, row in gdf_iat.iterrows():
    color = iat_color_map.get(row['IAT_class'], 'grey')
    gpd.GeoDataFrame([row], geometry='geometry', crs=gdf_sub.crs).plot(
        ax=ax, color=color, edgecolor='black', linewidth=1.2, alpha=0.75, zorder=3
    )
    cx, cy = row.geometry.centroid.x, row.geometry.centroid.y
    ax.text(cx, cy, f"{row['basin_id']}\nIAT={row['IAT']:.2f}",
            ha='center', va='center', fontsize=8, fontweight='bold',
            path_effects=[pe.withStroke(linewidth=2, foreground='white')])

legend_patches = [mpatches.Patch(color=c, label=l) for l, c in iat_color_map.items()]
ax.legend(handles=legend_patches, loc='lower left', fontsize=8,
          title='Tectonic Activity', title_fontsize=9, framealpha=0.9)
gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.6, alpha=0.5, zorder=5)
finalize_and_save(fig, ax, utm_ext, "10a_tectonic_IAT_map.png")

# â”€â”€ Plotly radar â€” tectonic scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig_r = go.Figure()
for i, bid in enumerate(df_IAT.index):
    row  = df_IAT.loc[bid]
    cats = ['AF_score','T_score','Vf_score','Smf_score']
    vals = [row['Score_AF'], row['Score_T'], row['Score_Vf'], row['Score_Smf']]
    vals += [vals[0]]
    fig_r.add_trace(go.Scatterpolar(
        r=vals, theta=['AF','T','Vf','Smf','AF'],
        fill='toself', name=bid,
        line_color=px.colors.qualitative.Set1[i % 9], opacity=0.65,
        hovertemplate=bid+'<br>%{theta}: %{r} (1=high 3=low activity)',
    ))
fig_r.update_layout(
    polar=dict(radialaxis=dict(range=[0, 3], tickvals=[1,2,3],
                               ticktext=['High','Moderate','Low'])),
    title="Tectonic Activity Score Radar â€” Per Subbasin",
    template='plotly_white', height=550,
)
save_fig(fig_r, "10b_tectonic_radar")

print("\nâœ… SECTION 10 complete.")

"""
=============================================================================
SECTION 10 â€” TECTONIC ACTIVITY INDICES
=============================================================================
Assumes Sections 1â€“3 variables are in memory:
  gdf_sub, df_master, DEM_ARR, DEM_TRANSFORM, DEM_RES,
  RASTERS, HILLSHADE, UTM_EPSG, FACC_ARR, SLOPE_ARR,
  gdf_streams, gdf_so, ORDER_COL, OUT_DIR, MAPS_DIR,
  PLOTS_DIR, TABLES_DIR, HTML_DIR

Parameters computed (per subbasin unless noted):
  AF   â€” Drainage Basin Asymmetry Factor (El Hamdouni et al., 2008)
  T    â€” Transverse Topographic Symmetry Factor (Cox, 1994)
  Vf   â€” Valley Floor Width-to-Height Ratio (Bull & McFadden, 1977)
  Smf  â€” Mountain Front Sinuosity (Bull & McFadden, 1977)
  IAT  â€” Index of Active Tectonics (composite classification)
  BS   â€” Basin Shape Index (Cannon, 1976)

References:
  Bull, W.B. & McFadden, L.D. (1977). Tectonic geomorphology N & S of the Garlock fault.
  Cox, R.T. (1994). Analysis of drainage basin symmetry as a rapid technique.
  El Hamdouni, R. et al. (2008). Assessment of relative active tectonics, SE Spain.
=============================================================================
"""

print("=" * 60)
print("SECTION 10 â€” TECTONIC ACTIVITY INDICES")
print("=" * 60)

from scipy.ndimage import label as ndlabel
from shapely.ops import split, unary_union
from shapely.geometry import LineString, MultiPoint

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HELPER â€” reuse map utilities from S4 (must already be in memory)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def raster_extent_from(bounds):
    return [bounds.left, bounds.right, bounds.bottom, bounds.top]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. ASYMMETRY FACTOR (AF)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AF = 100 Ã— (Ar / At)
# Ar = area of basin to the right of the trunk stream (facing downstream)
# At = total basin area
# AF = 50 â†’ no tectonic tilting; >50 or <50 â†’ tilting

print("\n[A] Asymmetry Factor (AF)...")

def compute_AF(basin_geom, stream_geom_longest):
    """
    Split basin by the projected midline of the trunk stream.
    Count cells left vs right.
    """
    try:
        # Project stream onto basin geometry: get bounding centroid axis
        cx = basin_geom.centroid.x
        cy = basin_geom.centroid.y
        # Use stream bearing to define left/right
        coords = list(stream_geom_longest.coords)
        if len(coords) < 2:
            return np.nan, np.nan, np.nan
        dx = coords[-1][0] - coords[0][0]
        dy = coords[-1][1] - coords[0][1]
        length = np.sqrt(dx**2 + dy**2)
        if length == 0:
            return np.nan, np.nan, np.nan
        # Perpendicular bisector through centroid
        perp_dx, perp_dy = -dy / length, dx / length
        scale = max(basin_geom.bounds[2] - basin_geom.bounds[0],
                    basin_geom.bounds[3] - basin_geom.bounds[1]) * 2
        bisector = LineString([
            (cx - perp_dx * scale, cy - perp_dy * scale),
            (cx + perp_dx * scale, cy + perp_dy * scale),
        ])
        parts = split(basin_geom, bisector)
        if len(parts.geoms) < 2:
            return np.nan, np.nan, np.nan
        At = basin_geom.area
        Ar = parts.geoms[1].area   # right side (facing downstream)
        Al = parts.geoms[0].area
        AF = 100 * (Ar / At)
        return AF, Ar / 1e6, Al / 1e6
    except Exception:
        return np.nan, np.nan, np.nan


AF_rows = []
for _, row in gdf_sub.iterrows():
    bid   = row['basin_id']
    geom  = row.geometry
    # Longest stream in basin
    segs  = gdf_so[gdf_so.geometry.within(geom.buffer(50))]
    if len(segs) == 0:
        AF_rows.append({'basin_id': bid, 'AF': np.nan,
                        'AF_deviation': np.nan, 'AF_class': 'Unknown'})
        continue
    longest = segs.loc[segs.geometry.length.idxmax()].geometry
    if longest.geom_type == 'MultiLineString':
        longest = max(longest.geoms, key=lambda g: g.length)
    AF_val, Ar, Al = compute_AF(geom, longest)
    AF_dev = abs(AF_val - 50) if not np.isnan(AF_val) else np.nan
    if np.isnan(AF_val):
        cls = 'Unknown'
    elif AF_dev < 5:
        cls = 'Symmetric (Low tectonic activity)'
    elif AF_dev < 15:
        cls = 'Slightly asymmetric (Moderate)'
    else:
        cls = 'Highly asymmetric (High tectonic activity)'
    AF_rows.append({'basin_id': bid, 'AF': round(AF_val, 3),
                    'AF_deviation': round(AF_dev, 3), 'AF_class': cls})
    print(f"  {bid}: AF={AF_val:.2f} | dev={AF_dev:.2f} | {cls}")

df_AF = pd.DataFrame(AF_rows).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. TRANSVERSE TOPOGRAPHIC SYMMETRY FACTOR (T)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# T = Da / Dd
# Da = distance from midline of basin to midline of active meander belt
# Dd = distance from midline of basin to basin divide
# T â†’ 0 : perfectly symmetric; T â†’ 1 : highly asymmetric

print("\n[B] Transverse Topographic Symmetry Factor (T)...")

def compute_T(basin_geom, trunk_stream_geom):
    """
    Approximation using centroid offset:
    T = distance(basin_centroid â†’ stream_centroid) /
        distance(basin_centroid â†’ furthest point on divide)
    """
    try:
        basin_c   = basin_geom.centroid
        stream_c  = trunk_stream_geom.centroid
        Da        = basin_c.distance(stream_c)
        # Dd: mean distance from centroid to boundary
        boundary_pts = [
            basin_geom.boundary.interpolate(basin_geom.boundary.length * f)
            for f in np.linspace(0, 1, 200)
        ]
        dists_to_bdy = [basin_c.distance(p) for p in boundary_pts]
        Dd = np.mean(dists_to_bdy)
        T  = Da / Dd if Dd > 0 else np.nan
        return T, Da, Dd
    except Exception:
        return np.nan, np.nan, np.nan


T_rows = []
for _, row in gdf_sub.iterrows():
    bid  = row['basin_id']
    geom = row.geometry
    segs = gdf_so[gdf_so.geometry.within(geom.buffer(50))]
    if len(segs) == 0:
        T_rows.append({'basin_id': bid, 'T': np.nan, 'T_class': 'Unknown'})
        continue
    longest = segs.loc[segs.geometry.length.idxmax()].geometry
    if longest.geom_type == 'MultiLineString':
        longest = max(longest.geoms, key=lambda g: g.length)
    T_val, Da, Dd = compute_T(geom, longest)
    cls = ('Symmetric' if T_val < 0.1 else
           'Slightly asymmetric' if T_val < 0.25 else
           'Moderately asymmetric' if T_val < 0.5 else
           'Highly asymmetric') if not np.isnan(T_val) else 'Unknown'
    T_rows.append({'basin_id': bid, 'T': round(T_val, 4) if not np.isnan(T_val) else np.nan,
                   'Da_m': round(Da, 1), 'Dd_m': round(Dd, 1), 'T_class': cls})
    print(f"  {bid}: T={T_val:.4f} | Da={Da:.0f}m | Dd={Dd:.0f}m | {cls}")

df_T = pd.DataFrame(T_rows).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. VALLEY FLOOR WIDTH-TO-HEIGHT RATIO (Vf)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Vf = 2Vfw / [(Eld - Esc) + (Erd - Esc)]
# Vfw  = valley floor width
# Eld/Erd = elevation of left/right valley walls at top
# Esc = elevation of valley floor

print("\n[C] Valley Floor Width-to-Height Ratio (Vf)...")

def compute_Vf_at_outlet(basin_geom, dem_path, n_transects=5, transect_frac=0.15):
    """
    Sample cross-valley transects near the outlet.
    Returns mean Vf across transects.
    """
    Vf_vals = []
    bounds   = basin_geom.bounds
    y_sample = np.linspace(bounds[1] + (bounds[3]-bounds[1])*0.05,
                           bounds[1] + (bounds[3]-bounds[1])*transect_frac,
                           n_transects)
    x_min, x_max = bounds[0], bounds[2]

    with rasterio.open(dem_path) as src:
        for y in y_sample:
            # horizontal transect
            pts_x  = np.linspace(x_min, x_max, 100)
            elevs  = []
            for px in pts_x:
                try:
                    r_i, c_i = rowcol(src.transform, px, y)
                    e = src.read(1)[r_i, c_i]
                    nd = src.nodata if src.nodata else -9999
                    elevs.append(np.nan if e == nd else float(e))
                except:
                    elevs.append(np.nan)
            elevs = np.array(elevs)
            valid  = ~np.isnan(elevs)
            if valid.sum() < 10:
                continue
            Esc    = np.nanmin(elevs)           # valley floor elevation
            Eld    = np.nanpercentile(elevs, 95) # left wall (approx)
            Erd    = Eld                         # symmetric approximation
            # Vfw: width of cells within 10% above minimum
            threshold = Esc + (Eld - Esc) * 0.10
            Vfw_cells = np.sum(elevs[valid] <= threshold)
            cell_size  = (x_max - x_min) / 100
            Vfw_m      = Vfw_cells * cell_size
            denom      = (Eld - Esc) + (Erd - Esc)
            if denom > 0:
                Vf_vals.append((2 * Vfw_m) / denom)

    return np.nanmean(Vf_vals) if Vf_vals else np.nan


Vf_rows = []
for _, row in gdf_sub.iterrows():
    bid   = row['basin_id']
    geom  = row.geometry
    Vf    = compute_Vf_at_outlet(geom, RASTERS['dem'])
    cls   = ('V-shaped valley (active uplift)' if not np.isnan(Vf) and Vf < 0.5 else
             'Transitional' if not np.isnan(Vf) and Vf < 1.0 else
             'Wide flat valley (tectonic quiescence)') if not np.isnan(Vf) else 'Unknown'
    Vf_rows.append({'basin_id': bid, 'Vf': round(Vf, 4) if not np.isnan(Vf) else np.nan,
                    'Vf_class': cls})
    print(f"  {bid}: Vf={Vf:.3f} | {cls}" if not np.isnan(Vf) else f"  {bid}: Vf=N/A")

df_Vf = pd.DataFrame(Vf_rows).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. MOUNTAIN FRONT SINUOSITY (Smf)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Smf = Lmf / Ls
# Lmf = length of mountain front (actual, sinuous boundary)
# Ls  = straight-line length of mountain front
# Smf â†’ 1 : straight, active front; Smf > 3 : sinuous, inactive

print("\n[D] Mountain Front Sinuosity (Smf)...")

def compute_Smf(basin_geom):
    """
    Use the lower boundary segment of each subbasin as mountain front proxy.
    Lmf = actual perimeter of lower 25% of basin extent
    Ls  = straight-line distance of same extent
    """
    try:
        bounds   = basin_geom.bounds
        y_thresh = bounds[1] + (bounds[3] - bounds[1]) * 0.25
        lower    = basin_geom.intersection(
            box(bounds[0], bounds[1], bounds[2], y_thresh)
        )
        if lower.is_empty:
            return np.nan
        Lmf = lower.boundary.length if hasattr(lower, 'boundary') else lower.length
        Ls  = np.sqrt((bounds[2] - bounds[0])**2)  # E-W extent of lower portion
        return Lmf / Ls if Ls > 0 else np.nan
    except Exception:
        return np.nan


Smf_rows = []
for _, row in gdf_sub.iterrows():
    bid = row['basin_id']
    Smf = compute_Smf(row.geometry)
    cls = ('Straight/active front' if not np.isnan(Smf) and Smf < 1.4 else
           'Moderately sinuous' if not np.isnan(Smf) and Smf < 3.0 else
           'Highly sinuous/inactive') if not np.isnan(Smf) else 'Unknown'
    Smf_rows.append({'basin_id': bid, 'Smf': round(Smf, 4) if not np.isnan(Smf) else np.nan,
                     'Smf_class': cls})
    print(f"  {bid}: Smf={Smf:.3f} | {cls}" if not np.isnan(Smf) else f"  {bid}: Smf=N/A")

df_Smf = pd.DataFrame(Smf_rows).set_index('basin_id')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  E. INDEX OF ACTIVE TECTONICS (IAT) â€” composite
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# IAT = mean of individual class scores (1=high, 2=moderate, 3=low activity)
# Following El Hamdouni et al. (2008)

print("\n[E] Index of Active Tectonics (IAT)...")

def score_AF(AF):
    if np.isnan(AF): return 2
    dev = abs(AF - 50)
    if dev > 15: return 1
    if dev > 5:  return 2
    return 3

def score_T(T):
    if np.isnan(T): return 2
    if T > 0.5:  return 1
    if T > 0.25: return 2
    return 3

def score_Vf(Vf):
    if np.isnan(Vf): return 2
    if Vf < 0.5: return 1
    if Vf < 1.0: return 2
    return 3

def score_Smf(Smf):
    if np.isnan(Smf): return 2
    if Smf < 1.4: return 1
    if Smf < 3.0: return 2
    return 3

def iat_class(iat):
    if iat <= 1.5: return "Class 1 â€” Very High"
    if iat <= 2.0: return "Class 2 â€” High"
    if iat <= 2.5: return "Class 3 â€” Moderate"
    return "Class 4 â€” Low"

IAT_rows = []
for bid in gdf_sub['basin_id']:
    AF_v  = df_AF.loc[bid, 'AF']     if bid in df_AF.index  else np.nan
    T_v   = df_T.loc[bid, 'T']       if bid in df_T.index   else np.nan
    Vf_v  = df_Vf.loc[bid, 'Vf']     if bid in df_Vf.index  else np.nan
    Smf_v = df_Smf.loc[bid, 'Smf']   if bid in df_Smf.index else np.nan

    s_AF, s_T, s_Vf, s_Smf = score_AF(AF_v), score_T(T_v), score_Vf(Vf_v), score_Smf(Smf_v)
    IAT   = np.mean([s_AF, s_T, s_Vf, s_Smf])
    cls   = iat_class(IAT)
    IAT_rows.append({
        'basin_id': bid,
        'AF': AF_v, 'T': T_v, 'Vf': Vf_v, 'Smf': Smf_v,
        'Score_AF': s_AF, 'Score_T': s_T, 'Score_Vf': s_Vf, 'Score_Smf': s_Smf,
        'IAT': round(IAT, 3), 'IAT_class': cls,
    })
    print(f"  {bid}: IAT={IAT:.2f} â†’ {cls}")

df_IAT = pd.DataFrame(IAT_rows).set_index('basin_id')
df_IAT.to_csv(os.path.join(TABLES_DIR, "tectonic_IAT.csv"))
print(f"\n  âœ… IAT table saved")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  F. TECTONIC MAP â€” IAT choropleth on hillshade
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[F] Generating Tectonic Activity Map...")

iat_color_map = {
    'Class 1 â€” Very High': '#d73027',
    'Class 2 â€” High'     : '#fc8d59',
    'Class 3 â€” Moderate' : '#fee08b',
    'Class 4 â€” Low'      : '#91bfdb',
}

gdf_iat = gdf_sub.merge(df_IAT[['IAT','IAT_class']].reset_index(), on='basin_id')

utm_ext  = compute_utm_extent()
fig, ax, utm_ext  = base_axes("Index of Active Tectonics (IAT) â€” El Hamdouni et al., 2008")

for _, row in gdf_iat.iterrows():
    color = iat_color_map.get(row['IAT_class'], 'grey')
    gpd.GeoDataFrame([row], geometry='geometry', crs=gdf_sub.crs).plot(
        ax=ax, color=color, edgecolor='black', linewidth=1.2, alpha=0.75, zorder=3
    )
    cx, cy = row.geometry.centroid.x, row.geometry.centroid.y
    ax.text(cx, cy, f"{row['basin_id']}\nIAT={row['IAT']:.2f}",
            ha='center', va='center', fontsize=8, fontweight='bold',
            path_effects=[pe.withStroke(linewidth=2, foreground='white')])

legend_patches = [mpatches.Patch(color=c, label=l) for l, c in iat_color_map.items()]
ax.legend(handles=legend_patches, loc='lower left', fontsize=8,
          title='Tectonic Activity', title_fontsize=9, framealpha=0.9)
gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.6, alpha=0.5, zorder=5)
finalize_and_save(fig, ax, utm_ext, "10a_tectonic_IAT_map.png")

# â”€â”€ Plotly radar â€” tectonic scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig_r = go.Figure()
for i, bid in enumerate(df_IAT.index):
    row  = df_IAT.loc[bid]
    cats = ['AF_score','T_score','Vf_score','Smf_score']
    vals = [row['Score_AF'], row['Score_T'], row['Score_Vf'], row['Score_Smf']]
    vals += [vals[0]]
    fig_r.add_trace(go.Scatterpolar(
        r=vals, theta=['AF','T','Vf','Smf','AF'],
        fill='toself', name=bid,
        line_color=px.colors.qualitative.Set1[i % 9], opacity=0.65,
        hovertemplate=bid+'<br>%{theta}: %{r} (1=high 3=low activity)',
    ))
fig_r.update_layout(
    polar=dict(radialaxis=dict(range=[0, 3], tickvals=[1,2,3],
                               ticktext=['High','Moderate','Low'])),
    title="Tectonic Activity Score Radar â€” Per Subbasin",
    template='plotly_white', height=550,
)
save_fig(fig_r, "10b_tectonic_radar")

print("\nâœ… SECTION 10 complete.")

"""
=============================================================================
SECTION 11 â€” GEOMORPHIC INDICES: SL, SPI, TWI
=============================================================================
Assumes Sections 1â€“10 variables are in memory.

Calculates:
  â€¢ Stream Length-gradient Index (SL Index)
  â€¢ Stream Power Index (SPI)
  â€¢ Topographic Wetness Index (TWI)

And prepares gdf_SL (GeoDataFrame with stream segments and SL anomalies)
and TWI_ARR (raster of TWI values).
=============================================================================
"""

print("=" * 60)
print("SECTION 11 â€” GEOMORPHIC INDICES: SL, SPI, TWI")
print("=" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  HELPER FUNCTIONS (for SL, SPI, TWI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def calculate_sl_index(stream_gdf, dem_arr, dem_transform, k=10):
    """
    Calculate Stream Length-gradient (SL) index for each stream segment.
    SL = (dH/dL) * L.
    dH/dL is local gradient, L is total channel length upstream (approximated).
    The local gradient is calculated over a window of k cells.
    """
    sl_indices = []
    for idx, row in stream_gdf.iterrows():
        geom = row.geometry
        if geom.geom_type == 'MultiLineString':
            geom = max(geom.geoms, key=lambda g: g.length)
        if geom.geom_type != 'LineString' or geom.length == 0:
            sl_indices.append(np.nan)
            continue

        coords = list(geom.coords)
        if len(coords) < 2:
            sl_indices.append(np.nan)
            continue

        # Sample elevation along the stream
        elevations = []
        for p in coords:
            row_idx, col_idx = rasterio.transform.rowcol(dem_transform, p[0], p[1])
            if 0 <= row_idx < dem_arr.shape[0] and 0 <= col_idx < dem_arr.shape[1]:
                elevations.append(dem_arr[row_idx, col_idx])
            else:
                elevations.append(np.nan)
        elevations = np.array(elevations)

        # Remove NaNs and corresponding coordinates
        valid_indices = ~np.isnan(elevations)
        elevations_valid = elevations[valid_indices]
        coords_valid     = np.array(coords)[valid_indices]

        if len(elevations_valid) < 2:
            sl_indices.append(np.nan)
            continue

        # Calculate local gradient (dH/dL) over a window of k points
        gradients = []
        for i in range(len(elevations_valid) - k):
            p1 = coords_valid[i]
            p2 = coords_valid[i+k]
            dist = np.sqrt((p2[0]-p1[0])**2 + (p2[1]-p1[1])**2)
            if dist > 0:
                grad = (elevations_valid[i] - elevations_valid[i+k]) / dist
                gradients.append(grad)

        if not gradients:
            sl_indices.append(np.nan)
            continue

        local_gradient = np.nanmean(gradients) # Average gradient over segment

        # Approximate upstream length as the total length of the segment
        # A more rigorous approach would trace upstream from the pour point
        L_upstream = geom.length # Approximation

        sl_index = local_gradient * L_upstream
        sl_indices.append(sl_index)
    return sl_indices

def calculate_spi(stream_gdf, flow_acc_arr, slope_arr, dem_res, threshold=1e-6):
    """
    Calculate Stream Power Index (SPI) for each stream segment.
    SPI = As * tan(beta), where As is contributing area, beta is slope.
    Approximates As with flow accumulation * cell_area.
    """
    spi_values = []
    for idx, row in stream_gdf.iterrows():
        geom = row.geometry
        if geom.geom_type == 'MultiLineString':
            geom = max(geom.geoms, key=lambda g: g.length)
        if geom.geom_type != 'LineString' or geom.length == 0:
            spi_values.append(np.nan)
            continue

        coords = list(geom.coords)
        fa_values = []
        slope_values = []

        for p in coords:
            row_idx, col_idx = rasterio.transform.rowcol(DEM_TRANSFORM, p[0], p[1])
            if 0 <= row_idx < flow_acc_arr.shape[0] and 0 <= col_idx < flow_acc_arr.shape[1]:
                fa_values.append(flow_acc_arr[row_idx, col_idx])
                slope_values.append(slope_arr[row_idx, col_idx])
            else:
                fa_values.append(np.nan)
                slope_values.append(np.nan)

        fa_values = np.array(fa_values)
        slope_values = np.array(slope_values)

        valid_indices = ~np.isnan(fa_values) & ~np.isnan(slope_values)
        if not np.any(valid_indices):
            spi_values.append(np.nan)
            continue

        # Use mean flow accumulation and slope for the segment
        mean_fa    = np.nanmean(fa_values[valid_indices])
        mean_slope = np.nanmean(slope_values[valid_indices]) # in degrees

        # Convert slope to radians for tan function
        mean_slope_rad = np.radians(mean_slope)

        # As (contributing area) = flow_accumulation * cell_area
        cell_area = dem_res * dem_res # m^2
        As = mean_fa * cell_area

        # Avoid division by zero or tan(90 deg)
        tan_beta = np.tan(mean_slope_rad)
        if tan_beta < threshold: # Set a small threshold for very flat areas
            tan_beta = threshold

        spi = As * tan_beta
        spi_values.append(spi)

    return spi_values

def calculate_sti(stream_gdf, flow_acc_arr, slope_arr, dem_res, threshold=1e-6):
    """
    Calculate Sediment Transport Index (STI) for each stream segment.
    STI = (As * sin(beta)). Simplified version for segments.
    Approximates As with flow accumulation * cell_area.
    """
    sti_values = []
    for idx, row in stream_gdf.iterrows():
        geom = row.geometry
        if geom.geom_type == 'MultiLineString':
            geom = max(geom.geoms, key=lambda g: g.length)
        if geom.geom_type != 'LineString' or geom.length == 0:
            sti_values.append(np.nan)
            continue

        coords = list(geom.coords)
        fa_values = []
        slope_values = []

        for p in coords:
            row_idx, col_idx = rasterio.transform.rowcol(DEM_TRANSFORM, p[0], p[1])
            if 0 <= row_idx < flow_acc_arr.shape[0] and 0 <= col_idx < flow_acc_arr.shape[1]:
                fa_values.append(flow_acc_arr[row_idx, col_idx])
                slope_values.append(slope_arr[row_idx, col_idx])
            else:
                fa_values.append(np.nan)
                slope_values.append(np.nan)

        fa_values = np.array(fa_values)
        slope_values = np.array(slope_values)

        valid_indices = ~np.isnan(fa_values) & ~np.isnan(slope_values)
        if not np.any(valid_indices):
            sti_values.append(np.nan)
            continue

        mean_fa = np.nanmean(fa_values[valid_indices])
        mean_slope = np.nanmean(slope_values[valid_indices]) # in degrees

        mean_slope_rad = np.radians(mean_slope)
        As = mean_fa * dem_res # Specific catchment area (m) for unit contour length (simplified)

        sin_beta = np.sin(mean_slope_rad)
        if sin_beta < threshold:
            sin_beta = threshold

        sti = As * sin_beta
        sti_values.append(sti)

    return sti_values

def calculate_twi(dem_arr, dem_res):
    """
    Calculate Topographic Wetness Index (TWI) raster using a simple approach.
    TWI = ln(As / tan(beta))
    As is specific catchment area (approximated by flow accumulation * cell size)
    tan(beta) is local slope.
    This is a simplified TWI calculation for demonstration. For precise TWI,
    a dedicated hydrological model (e.g., WhiteboxTools, pysheds) is needed.
    """
    # Use existing flow accumulation and slope arrays
    flow_acc_arr = FACC_ARR.copy()
    slope_arr    = SLOPE_ARR.copy() # already in degrees

    # Convert slope to radians
    slope_rad_arr = np.radians(slope_arr)

    # Calculate specific catchment area (As) - approximation
    # Assuming flow_acc_arr represents number of upstream cells
    As_arr = flow_acc_arr * dem_res  # Specific catchment area (m^2/m)

    # Avoid division by zero or tan(0)
    tan_beta_arr = np.tan(slope_rad_arr)
    # Set a minimum slope to avoid log of zero/negative and very high TWI values
    min_slope_rad = np.radians(0.01) # 0.01 degrees minimum slope
    tan_beta_arr[tan_beta_arr < np.tan(min_slope_rad)] = np.tan(min_slope_rad)

    # Calculate TWI
    twi_arr = np.log(As_arr / tan_beta_arr)

    # Mask out invalid values
    twi_arr[np.isnan(dem_arr)] = np.nan
    twi_arr[flow_acc_arr == 0] = np.nan # TWI is undefined for 0 flow accumulation

    return twi_arr.astype(np.float32)

def rasterize_segment_attribute(gdf, attribute_col, dem_arr_shape, dem_transform, nodata_val=-9999.0):
    """
    Rasterize a GeoDataFrame's attribute (from line segments) onto a raster grid.
    Values are burned along the line's path, taking the max value if multiple lines cross.
    """
    raster = np.full(dem_arr_shape, np.nan, dtype=np.float32)

    for _, row in gdf[gdf[attribute_col].notna()].iterrows():
        geom = row.geometry
        val  = row[attribute_col]
        if geom.geom_type == 'MultiLineString':
            for single_line in geom.geoms:
                for x, y in single_line.coords:
                    r_idx, c_idx = rasterio.transform.rowcol(dem_transform, x, y)
                    if 0 <= r_idx < dem_arr_shape[0] and 0 <= c_idx < dem_arr_shape[1]:
                        if np.isnan(raster[r_idx, c_idx]):
                            raster[r_idx, c_idx] = val
                        else:
                            raster[r_idx, c_idx] = max(raster[r_idx, c_idx], val)
        elif geom.geom_type == 'LineString':
            for x, y in geom.coords:
                r_idx, c_idx = rasterio.transform.rowcol(dem_transform, x, y)
                if 0 <= r_idx < dem_arr_shape[0] and 0 <= c_idx < dem_arr_shape[1]:
                    if np.isnan(raster[r_idx, c_idx]):
                        raster[r_idx, c_idx] = val
                    else:
                        raster[r_idx, c_idx] = max(raster[r_idx, c_idx], val)

    # Fill remaining NaNs with nodata_val for rasterio compatibility
    raster[np.isnan(raster)] = nodata_val
    return raster.astype(np.float32)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. STREAM LENGTH-GRADIENT (SL) INDEX
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[A] Computing Stream Length-gradient (SL) Index...")

# Join stream segments to subbasins for basin_id access
gdf_so_with_basin_id = gpd.sjoin(
    gdf_so.copy(), gdf_sub[['basin_id', 'geometry']], how='left', predicate='intersects'
).drop(columns=['index_right']).dropna(subset=['basin_id'])

# Ensure basin_id is set correctly for all segments
if 'basin_id' not in gdf_so_with_basin_id.columns:
    # Fallback if sjoin fails to assign basin_id consistently
    gdf_so_with_basin_id['basin_id'] = None
    for bid in gdf_sub['basin_id'].unique():
        basin_geom = gdf_sub[gdf_sub['basin_id'] == bid].geometry.iloc[0]
        # Find streams within this basin
        streams_in_basin = gdf_so_with_basin_id.geometry.apply(lambda x: x.intersects(basin_geom))
        gdf_so_with_basin_id.loc[streams_in_basin, 'basin_id'] = bid

# Calculate SL index for each segment
# Using gdf_so (stream order segments) as the base for SL calculations
gdf_SL = gdf_so_with_basin_id.copy()
gdf_SL['SL_index'] = calculate_sl_index(gdf_SL, DEM_ARR, DEM_TRANSFORM)

# Calculate SL anomaly (deviation from mean for its order)
mean_sl_per_order = gdf_SL.groupby(ORDER_COL)['SL_index'].mean()
std_sl_per_order  = gdf_SL.groupby(ORDER_COL)['SL_index'].std()

gdf_SL['mean_SL_order'] = gdf_SL[ORDER_COL].map(mean_sl_per_order)
gdf_SL['std_SL_order']  = gdf_SL[ORDER_COL].map(std_sl_per_order)

# SL anomaly is deviation from mean SL for its order, normalized by std dev
gdf_SL['SL_anomaly'] = (gdf_SL['SL_index'] - gdf_SL['mean_SL_order']) / gdf_SL['std_SL_order']

# Replace inf/-inf with nan for anomaly calculation
gdf_SL['SL_anomaly'] = gdf_SL['SL_anomaly'].replace([np.inf, -np.inf], np.nan)

# Store max SL anomaly per basin for plotting/summary later
SL_per_basin = gdf_SL.groupby('basin_id')['SL_anomaly'].agg(
    SL_anomaly_mean='mean', SL_anomaly_max='max', SL_anomaly_std='std'
).round(4)

print("  SL Anomaly per basin (mean/max):")
print(SL_per_basin.to_string())
SL_per_basin.to_csv(os.path.join(TABLES_DIR, "sl_anomaly_per_basin.csv"))
gdf_SL.to_file(os.path.join(SHAPES_DIR, "streams_sl_anomaly.shp"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. STREAM POWER INDEX (SPI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[B] Computing Stream Power Index (SPI)...")

gdf_SL['SPI'] = calculate_spi(gdf_SL, FACC_ARR, SLOPE_ARR, DEM_RES)

SPI_per_basin = gdf_SL.groupby('basin_id')['SPI'].agg(
    SPI_mean='mean', SPI_max='max', SPI_std='std'
).round(4)
print("  SPI per basin (mean/max):")
print(SPI_per_basin.to_string())
SPI_per_basin.to_csv(os.path.join(TABLES_DIR, "spi_per_basin.csv"))

# Rasterize SPI
SPI_ARR = rasterize_segment_attribute(gdf_SL, 'SPI', DEM_ARR.shape, DEM_TRANSFORM)
save_raster(SPI_ARR, os.path.join(OUT_DIR, "spi.tif"), RASTERS['dem'])
RASTERS['spi'] = os.path.join(OUT_DIR, "spi.tif")
print(f"  SPI raster range: {np.nanmin(SPI_ARR):.3f} â€“ {np.nanmax(SPI_ARR):.3f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. SEDIMENT TRANSPORT INDEX (STI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[C] Computing Sediment Transport Index (STI)...")

gdf_SL['STI'] = calculate_sti(gdf_SL, FACC_ARR, SLOPE_ARR, DEM_RES)

STI_per_basin = gdf_SL.groupby('basin_id')['STI'].agg(
    STI_mean='mean', STI_max='max', STI_std='std'
).round(4)
print("  STI per basin (mean/max):")
print(STI_per_basin.to_string())
STI_per_basin.to_csv(os.path.join(TABLES_DIR, "sti_per_basin.csv"))

# Rasterize STI
STI_ARR = rasterize_segment_attribute(gdf_SL, 'STI', DEM_ARR.shape, DEM_TRANSFORM)
save_raster(STI_ARR, os.path.join(OUT_DIR, "sti.tif"), RASTERS['dem'])
RASTERS['sti'] = os.path.join(OUT_DIR, "sti.tif")
print(f"  STI raster range: {np.nanmin(STI_ARR):.3f} â€“ {np.nanmax(STI_ARR):.3f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. TOPOGRAPHIC WETNESS INDEX (TWI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[D] Computing Topographic Wetness Index (TWI)...")

TWI_ARR = calculate_twi(DEM_ARR, DEM_RES)
save_raster(TWI_ARR, os.path.join(OUT_DIR, "twi.tif"), RASTERS['dem'])
RASTERS['twi'] = os.path.join(OUT_DIR, "twi.tif")
print(f"  TWI range: {np.nanmin(TWI_ARR):.3f} â€“ {np.nanmax(TWI_ARR):.3f}")

# Per-basin TWI statistics
TWI_basin = []
for _, row in gdf_sub.iterrows():
    geom = [row.geometry.__geo_interface__]
    with rasterio.open(os.path.join(OUT_DIR, "twi.tif")) as src:
        try:
            arr_m, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
            twi_clip  = arr_m[0]
            twi_clip[twi_clip == -9999] = np.nan
        except:
            twi_clip  = TWI_ARR.copy()
    TWI_basin.append({
        'basin_id': row['basin_id'],
        'TWI_mean': round(float(np.nanmean(twi_clip)), 4),
        'TWI_max' : round(float(np.nanmax(twi_clip)), 4),
        'TWI_std': round(float(np.nanstd(twi_clip)), 4),
    })
df_TWI_basin = pd.DataFrame(TWI_basin).set_index('basin_id')
print("  Per-basin TWI:")
print(df_TWI_basin.to_string())
df_TWI_basin.to_csv(os.path.join(TABLES_DIR, "twi_per_basin.csv"))

print("\nâœ… SECTION 11 complete.")

print("=" * 60)
print("SECTION 12 â€” GEOMORPHIC ANOMALY & LINEAMENT ANALYSIS")
print("=" * 60)

from scipy.ndimage import (sobel, gaussian_filter, maximum_filter,
                            generic_filter, binary_dilation)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. CHANNEL SINUOSITY INDEX (SI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SI = actual channel length / straight-line distance between endpoints
# SI > 1.5 â†’ sinuous/meandering; SI â‰ˆ 1.0 â†’ straight

print("\n[A] Channel Sinuosity Index (SI)...")

def compute_sinuosity(geom):
    """SI = channel length / straight-line distance between endpoints."""
    if geom.geom_type == 'MultiLineString':
        geom = max(geom.geoms, key=lambda g: g.length)
    if geom.geom_type != 'LineString' or geom.length < DEM_RES:
        return np.nan
    coords    = list(geom.coords)
    straight  = np.sqrt((coords[-1][0] - coords[0][0])**2 +
                        (coords[-1][1] - coords[0][1])**2)
    return geom.length / straight if straight > 0 else np.nan


gdf_SL['SI'] = gdf_SL['geometry'].apply(compute_sinuosity)
SI_per_basin  = gdf_SL.groupby('basin_id')['SI'].agg(
    SI_mean='mean', SI_max='max', SI_std='std'
).round(4)

def si_class(si):
    if np.isnan(si):  return 'Unknown'
    if si < 1.05:     return 'Straight (structural control)'
    if si < 1.3:      return 'Irregular'
    if si < 1.5:      return 'Sinuous'
    return 'Meandering'

SI_per_basin['SI_class'] = SI_per_basin['SI_mean'].apply(si_class)
print(SI_per_basin.to_string())
SI_per_basin.to_csv(os.path.join(TABLES_DIR, "sinuosity_per_basin.csv"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. GEOMORPHIC ANOMALY INDEX (GAI) RASTER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GAI = normalised composite of:
#   â€¢ SL anomaly (rasterised from segment values)
#   â€¢ TRI (terrain ruggedness)
#   â€¢ TWI inverted (low TWI = steep, anomalous)
# Higher GAI â†’ geomorphically anomalous zone

print("\n[B] Geomorphic Anomaly Index (GAI) raster...")

def normalise_0_1(arr):
    mn, mx = np.nanmin(arr), np.nanmax(arr)
    if mx == mn:
        return np.zeros_like(arr)
    return (arr - mn) / (mx - mn)

# Rasterise SL anomaly: burn each segment's SL_anomaly value onto raster
SL_anomaly_raster = np.full(DEM_ARR.shape, np.nan, dtype=np.float32)
with rasterio.open(RASTERS['dem']) as src:
    transform_r = src.transform
    for _, seg in gdf_SL[gdf_SL['SL_anomaly'].notna()].iterrows():
        geom = seg['geometry']
        pts  = [geom.interpolate(f, normalized=True) for f in np.linspace(0, 1, 20)]
        for pt in pts:
            try:
                r_i, c_i = rowcol(transform_r, pt.x, pt.y)
                if 0 <= r_i < SL_anomaly_raster.shape[0] and 0 <= c_i < SL_anomaly_raster.shape[1]:
                    existing = SL_anomaly_raster[r_i, c_i]
                    val      = seg['SL_anomaly']
                    SL_anomaly_raster[r_i, c_i] = val if np.isnan(existing) else max(existing, val)
            except:
                pass

# Fill gaps with Gaussian spread (proximity decay)
mask_sl = ~np.isnan(SL_anomaly_raster)
SL_filled = np.where(mask_sl, SL_anomaly_raster, 0)
SL_spread = gaussian_filter(SL_filled, sigma=5)
SL_spread[np.isnan(DEM_ARR)] = np.nan

# TRI already computed: TRI_ARR
# TWI inverted: high TWI = flat = low anomaly â†’ invert
TWI_inv = np.nanmax(TWI_ARR) - TWI_ARR

# Composite GAI
n_SL  = normalise_0_1(SL_spread)
n_TRI = normalise_0_1(TRI_ARR)
n_TWI = normalise_0_1(TWI_inv)

GAI = (n_SL * 0.5 + n_TRI * 0.3 + n_TWI * 0.2)
GAI[np.isnan(DEM_ARR)] = np.nan

save_raster(GAI, os.path.join(OUT_DIR, "GAI.tif"), RASTERS['dem'])
RASTERS['GAI'] = os.path.join(OUT_DIR, "GAI.tif")
print(f"  GAI range: {np.nanmin(GAI):.3f} â€“ {np.nanmax(GAI):.3f}")

# Classify high anomaly zones (top 20%)
GAI_thresh       = np.nanpercentile(GAI, 80)
HIGH_ANOMALY     = (GAI > GAI_thresh).astype(np.float32)
HIGH_ANOMALY[np.isnan(DEM_ARR)] = np.nan
save_raster(HIGH_ANOMALY, os.path.join(OUT_DIR, "GAI_high_anomaly.tif"), RASTERS['dem'])

# Per-basin GAI statistics
GAI_basin = []
for _, row in gdf_sub.iterrows():
    geom = [row.geometry.__geo_interface__]
    with rasterio.open(os.path.join(OUT_DIR, "GAI.tif")) as src:
        try:
            arr_m, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
            gai_clip  = arr_m[0]
            gai_clip[gai_clip == -9999] = np.nan
        except:
            gai_clip  = GAI.copy()
    GAI_basin.append({
        'basin_id': row['basin_id'],
        'GAI_mean': round(float(np.nanmean(gai_clip)), 4),
        'GAI_max' : round(float(np.nanmax(gai_clip)), 4),
        'GAI_high_frac': round(float(np.nanmean(gai_clip > GAI_thresh)), 4),
    })
df_GAI_basin = pd.DataFrame(GAI_basin).set_index('basin_id')
print("  Per-basin GAI:")
print(df_GAI_basin.to_string())
df_GAI_basin.to_csv(os.path.join(TABLES_DIR, "GAI_per_basin.csv"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. LINEAMENT PROXY â€” structural lineament detection
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Method: Sobel edge detection on smoothed DEM + slope, thresholded to
# identify linear high-gradient zones likely representing faults/fractures.

print("\n[C] Structural Lineament Proxy...")

try:
    from skimage.feature import canny
    from skimage.transform import probabilistic_hough_line
    SKIMAGE_OK = True
except ImportError:
    SKIMAGE_OK = False
    print("  scikit-image not available â€” using Sobel only")

# Smooth DEM
dem_smooth   = gaussian_filter(np.where(np.isnan(DEM_ARR), np.nanmean(DEM_ARR), DEM_ARR), sigma=3)

# Sobel edge magnitude
sx = sobel(dem_smooth, axis=1)
sy = sobel(dem_smooth, axis=0)
edge_mag = np.hypot(sx, sy)
edge_mag[np.isnan(DEM_ARR)] = 0

# Combine with slope for structural emphasis
edge_combined = (normalise_0_1(edge_mag) * 0.6 +
                 normalise_0_1(np.where(np.isnan(SLOPE_ARR), 0, SLOPE_ARR)) * 0.4)
edge_combined[np.isnan(DEM_ARR)] = np.nan
save_raster(edge_combined.astype(np.float32),
            os.path.join(OUT_DIR, "lineament_proxy.tif"), RASTERS['dem'])

# Detect probable lineaments using Canny + Hough if available
LINEAMENTS_GDF = None
if SKIMAGE_OK:
    try:
        edge_uint8 = ((edge_combined / np.nanmax(edge_combined)) * 255).astype(np.uint8)
        canny_edges = canny(edge_uint8, sigma=2,
                            low_threshold=50, high_threshold=100)
        lines = probabilistic_hough_line(
            canny_edges, threshold=30, line_length=20, line_gap=5
        )
        if lines:
            line_geoms = []
            with rasterio.open(RASTERS['dem']) as src:
                T = src.transform
                for (x0, y0), (x1, y1) in lines:
                    wx0, wy0 = xy(T, y0, x0)
                    wx1, wy1 = xy(T, y1, x1)
                    if abs(wx1 - wx0) > 0 or abs(wy1 - wy0) > 0:
                        line_geoms.append(LineString([(wx0, wy0), (wx1, wy1)]))
            if line_geoms:
                LINEAMENTS_GDF = gpd.GeoDataFrame(
                    {'lineament_id': range(len(line_geoms))},
                    geometry=line_geoms, crs=UTM_EPSG
                )
                LINEAMENTS_GDF.to_file(os.path.join(SHAPES_DIR, "lineament_proxy.shp"))
                print(f"  Detected {len(line_geoms)} probable lineaments")
    except Exception as e:
        print(f"  Hough detection failed ({e}) â€” edge raster saved only")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. MAPS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[D] Generating anomaly maps...")

utm_ext = compute_utm_extent()

# GAI map
fig, ax, utm_ext = base_axes("Geomorphic Anomaly Index (GAI)\n"
                    "(0.5Ã—SL + 0.3Ã—TRI + 0.2Ã—TWIâ»Â¹ normalised composite)")
im = ax.imshow(
    GAI, extent=raster_extent(), origin='upper',
    cmap='RdYlGn_r', alpha=0.80, zorder=1,
    vmin=0, vmax=1,
)
# High anomaly contour overlay
b = DEM_BOUNDS
x_c = np.linspace(b.left, b.right,  GAI.shape[1])
y_c = np.linspace(b.bottom, b.top,  GAI.shape[0])[::-1]
XX, YY = np.meshgrid(x_c, y_c)
ax.contour(XX, YY, np.where(np.isnan(GAI), 0, GAI),
           levels=[GAI_thresh], colors='black', linewidths=1.5,
           linestyles='--', zorder=8)
ax.text(0.02, 0.06, f"Dashed contour = top 20%\nGAI threshold = {GAI_thresh:.3f}",
        transform=ax.transAxes, fontsize=7.5, style='italic',
        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
gdf_sub.boundary.plot(ax=ax, edgecolor='black', linewidth=1.2, zorder=10)
gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.5, alpha=0.4, zorder=6)
divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="3%", pad=0.07)
cb  = plt.colorbar(im, cax=cax)
cb.set_label("GAI (0 = low, 1 = high anomaly)", fontsize=9)
finalize_and_save(fig, ax, utm_ext, "12a_GAI_map.png")

# Lineament proxy map
fig, ax, utm_ext = base_axes("Structural Lineament Proxy (Sobel edge + slope composite)")
im2 = ax.imshow(
    edge_combined,
    extent=raster_extent(), origin='upper',
    cmap='copper', alpha=0.80, zorder=1,
    vmin=0, vmax=np.nanpercentile(edge_combined, 99),
)
if LINEAMENTS_GDF is not None and len(LINEAMENTS_GDF) > 0:
    LINEAMENTS_GDF.plot(ax=ax, color='cyan', linewidth=0.7, alpha=0.7, zorder=7,
                        label='Probable lineaments')
    ax.legend(loc='lower left', fontsize=8, framealpha=0.85)
gdf_sub.boundary.plot(ax=ax, edgecolor='white', linewidth=1.2, zorder=10)
divider2 = make_axes_locatable(ax)
cax2 = divider2.append_axes("right", size="3%", pad=0.07)
cb2  = plt.colorbar(im2, cax=cax2)
cb2.set_label("Edge Magnitude (normalised)", fontsize=9)
finalize_and_save(fig, ax, utm_ext, "12b_lineament_proxy_map.png")

# Channel sinuosity map â€” coloured by SI
fig, ax, utm_ext = base_axes("Channel Sinuosity Index (SI) per Segment")
SI_valid  = gdf_SL[gdf_SL['SI'].notna()].copy()
if len(SI_valid) > 0:
    vmin_si, vmax_si = 1.0, np.nanpercentile(SI_valid['SI'], 98)
    cmap_si = plt.get_cmap('RdYlBu_r')
    norm_si = Normalize(vmin=vmin_si, vmax=vmax_si)
    for _, seg in SI_valid.iterrows():
        color = cmap_si(norm_si(seg['SI']))
        ax.plot(*seg.geometry.xy, color=color, linewidth=1.2, zorder=5)
    sm_si = plt.cm.ScalarMappable(cmap=cmap_si, norm=norm_si)
    sm_si.set_array([])
    cb3 = plt.colorbar(sm_si, ax=ax, fraction=0.03, pad=0.02)
    cb3.set_label("Sinuosity Index (SI)", fontsize=9)
gdf_sub.boundary.plot(ax=ax, edgecolor='black', linewidth=1.2, zorder=10)
finalize_and_save(fig, ax, utm_ext, "12c_sinuosity_map.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  E. PLOTLY â€” GAI interactive + anomaly scatter
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[E] Plotly charts...")

# GAI per basin bar + sinuosity overlay
fig = make_subplots(rows=1, cols=2,
                    subplot_titles=["GAI Mean per Subbasin",
                                    "Sinuosity vs SL Anomaly"])
fig.add_trace(go.Bar(
    x=df_GAI_basin.index.tolist(),
    y=df_GAI_basin['GAI_mean'].tolist(),
    marker_color=px.colors.sequential.Reds[3:],
    text=[f"{v:.3f}" for v in df_GAI_basin['GAI_mean']],
    textposition='outside',
    hovertemplate='%{x}<br>GAI mean: %{y:.3f}',
    name='GAI mean',
), row=1, col=1)
fig.add_trace(go.Bar(
    x=df_GAI_basin.index.tolist(),
    y=(df_GAI_basin['GAI_high_frac'] * 100).tolist(),
    marker_color=px.colors.sequential.OrRd[3:],
    name='% High anomaly',
    hovertemplate='%{x}<br>High anomaly: %{y:.1f}%',
    yaxis='y2',
), row=1, col=1)

# Sinuosity vs SL scatter
for bid in gdf_sub['basin_id']:
    si_m  = SI_per_basin.loc[bid, 'SI_mean'] if bid in SI_per_basin.index else np.nan
    sl_m  = SL_per_basin.loc[bid, 'SL_anomaly_max'] if bid in SL_per_basin.index else np.nan
    if np.isnan(si_m) or np.isnan(sl_m):
        continue
    fig.add_trace(go.Scatter(
        x=[si_m], y=[sl_m], mode='markers+text',
        text=[bid], textposition='top center',
        marker=dict(size=14, symbol='circle'),
        name=bid,
        hovertemplate=f"{bid}<br>SI={si_m:.3f}<br>SL anomaly={sl_m:.2f}",
    ), row=1, col=2)

fig.update_xaxes(title_text='Subbasin', row=1, col=1)
fig.update_yaxes(title_text='GAI Mean', row=1, col=1)
fig.update_xaxes(title_text='Mean Sinuosity (SI)', row=1, col=2)
fig.update_yaxes(title_text='Max SL Anomaly', row=1, col=2)
fig.update_layout(title="Geomorphic Anomaly Analysis",
                  template='plotly_white', height=480, showlegend=True)
save_fig(fig, "12d_geomorphic_anomaly_plotly")

print("\nâœ… SECTION 12 complete.")

print("=" * 60)
print("SECTION 13 â€” FLOOD HAZARD INDICATORS")
print("=" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  A. VERIFY TWI / SPI / STI (computed in S11)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[A] Loading TWI, SPI, STI arrays...")
assert 'twi' in RASTERS, "TWI raster not found â€” ensure Section 11 ran first"

# Re-read into memory (may have been computed in S11)
with rasterio.open(RASTERS['twi']) as src:
    TWI_ARR2 = src.read(1).astype(np.float32)
    TWI_ARR2[TWI_ARR2 == -9999.0] = np.nan

# Now, SPI and STI rasters should be available in RASTERS if Section 11 ran correctly
assert 'spi' in RASTERS, "SPI raster not found in RASTERS after Section 11"
assert 'sti' in RASTERS, "STI raster not found in RASTERS after Section 11"

with rasterio.open(RASTERS['spi']) as src:
    SPI_ARR2 = src.read(1).astype(np.float32)
    SPI_ARR2[SPI_ARR2 == -9999.0] = np.nan

with rasterio.open(RASTERS['sti']) as src:
    STI_ARR2 = src.read(1).astype(np.float32)
    STI_ARR2[STI_ARR2 == -9999.0] = np.nan

# Let's adjust the prints to reflect what is actually available.
print(f"  TWI : min={np.nanmin(TWI_ARR2):.2f} max={np.nanmax(TWI_ARR2):.2f} mean={np.nanmean(TWI_ARR2):.2f}")
print(f"  SPI : min={np.nanmin(SPI_ARR2):.2f} max={np.nanmax(SPI_ARR2):.2f} mean={np.nanmean(SPI_ARR2):.2f}")
print(f"  STI : min={np.nanmin(STI_ARR2):.2f} max={np.nanmax(STI_ARR2):.2f} mean={np.nanmean(STI_ARR2):.2f}")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  B. FLASH FLOOD POTENTIAL INDEX (FFPI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FFPI raster derived from slope, relief proxy, and TWI
# Weights following Smith (2003) and Gregory & Walling (1973) concepts

print("\n[B] Flash Flood Potential Index (FFPI)...")

def normalise_raster(arr):
    mn, mx = np.nanmin(arr), np.nanmax(arr)
    if mx == mn:
        return np.zeros_like(arr)
    return (arr - mn) / (mx - mn)


# Component normalised rasters
norm_slope   = normalise_raster(np.where(np.isnan(SLOPE_ARR), 0, SLOPE_ARR))

# Relief proxy: local relief within 5Ã—5 neighbourhood
from scipy.ndimage import maximum_filter, minimum_filter
dem_safe     = np.where(np.isnan(DEM_ARR), np.nanmean(DEM_ARR), DEM_ARR)
local_relief = maximum_filter(dem_safe, size=5) - minimum_filter(dem_safe, size=5)
local_relief[np.isnan(DEM_ARR)] = np.nan
norm_relief  = normalise_raster(np.where(np.isnan(local_relief), 0, local_relief))

# TWI inverted: high TWI = flat accumulation zone = high flood potential
TWI_safe     = np.where(np.isnan(TWI_ARR2), np.nanmin(TWI_ARR2), TWI_ARR2)
norm_twi     = normalise_raster(TWI_safe)

# SPI: high SPI = high stream power = high flood energy
# Using a placeholder for now, as SPI raster is not generated.
SPI_safe     = np.where(np.isnan(SPI_ARR2), 0, SPI_ARR2) # SPI_ARR2 is a nan placeholder
norm_spi     = normalise_raster(np.log1p(SPI_safe))  # log-transform

# Weighted FFPI
FFPI = (norm_slope  * 0.35 +
        norm_relief * 0.25 +
        norm_twi    * 0.25 +
        norm_spi    * 0.15) # This will be heavily influenced by NaNs from norm_spi
FFPI[np.isnan(DEM_ARR)] = np.nan

save_raster(FFPI.astype(np.float32), os.path.join(OUT_DIR, "FFPI.tif"), RASTERS['dem'])
RASTERS['FFPI'] = os.path.join(OUT_DIR, "FFPI.tif")
print(f"  FFPI range: {np.nanmin(FFPI):.3f} â€“ {np.nanmax(FFPI):.3f}")

# Classify FFPI
def classify_ffpi(val):
    if np.isnan(val): return "Unknown"
    if val > 0.75:    return "Very High"
    if val > 0.55:    return "High"
    if val > 0.35:    return "Moderate"
    if val > 0.20:    return "Low"
    return "Very Low"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  C. PER-BASIN HAZARD STATISTICS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[C] Per-basin hazard statistics...")

HAZARD_ROWS = []
for _, row in gdf_sub.iterrows():
    bid  = row['basin_id']
    geom = [row.geometry.__geo_interface__]

    def mask_raster(path):
        with rasterio.open(path) as src:
            try:
                arr_m, _ = rio_mask(src, geom, crop=True, nodata=np.nan)
                arr = arr_m[0].astype(np.float32)
                arr[arr == -9999.0] = np.nan
                return arr
            except:
                return np.array([np.nan])

    twi_clip  = mask_raster(RASTERS['twi']) # Use 'twi' (lowercase)
    spi_clip  = mask_raster(RASTERS['spi']) # SPI raster is now available
    sti_clip  = mask_raster(RASTERS['sti']) # STI raster is now available
    ffpi_clip = mask_raster(RASTERS['FFPI'])

    ffpi_mean = float(np.nanmean(ffpi_clip))
    HAZARD_ROWS.append({
        'basin_id'      : bid,
        'TWI_mean'      : round(float(np.nanmean(twi_clip)),  3),
        'TWI_max'       : round(float(np.nanmax(twi_clip)),   3),
        'SPI_mean'      : round(float(np.nanmean(spi_clip)),  3) if np.any(~np.isnan(spi_clip)) else np.nan,
        'SPI_max'       : round(float(np.nanmax(spi_clip)),   3) if np.any(~np.isnan(spi_clip)) else np.nan,
        'STI_mean'      : round(float(np.nanmean(sti_clip)),  3) if np.any(~np.isnan(sti_clip)) else np.nan,
        'STI_max'       : round(float(np.nanmax(sti_clip)),   3) if np.any(~np.isnan(sti_clip)) else np.nan,
        'FFPI_mean'     : round(ffpi_mean, 4),
        'FFPI_max'      : round(float(np.nanmax(ffpi_clip)),  4),
        'FFPI_high_frac': round(float(np.nanmean(ffpi_clip > 0.55)), 4),
        'FFPI_class'    : classify_ffpi(ffpi_mean),
    })
    print(f"  {bid}: TWI_mean={np.nanmean(twi_clip):.2f} | "
          f"SPI_mean={np.nanmean(spi_clip):.2f} | "
          f"FFPI_mean={ffpi_mean:.3f} â†’ {classify_ffpi(ffpi_mean)}")

df_hazard = pd.DataFrame(HAZARD_ROWS).set_index('basin_id')

# Composite Flood Hazard Rank
rank_cols = ['TWI_mean', 'SPI_mean', 'STI_mean', 'FFPI_mean']
df_hazard_r = df_hazard[rank_cols].copy()
for col in rank_cols:
    df_hazard[f'rank_{col}'] = df_hazard_r[col].rank(ascending=False, method='min')
df_hazard['FHI_rank'] = df_hazard[[f'rank_{c}' for c in rank_cols]].mean(axis=1)
df_hazard['FHI_priority'] = pd.qcut(
    df_hazard['FHI_rank'], q=3, labels=['High','Moderate','Low'], duplicates='drop'
)

df_hazard.to_csv(os.path.join(TABLES_DIR, "flood_hazard_indices.csv"))
print(f"\n  âœ… Flood hazard table saved")
print(df_hazard[['TWI_mean','SPI_mean','STI_mean','FFPI_mean','FFPI_class','FHI_priority']].to_string())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  D. MAPS â€” all 5 hazard maps
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[D] Generating hazard maps...")

utm_ext = compute_utm_extent()

MAP_CONFIGS = [
    ('TWI',  TWI_ARR2, 'Topographic Wetness Index (TWI)', 'Blues',   "13a_TWI_map.png"),
    ('SPI',  SPI_ARR2, 'Stream Power Index (SPI)',        'YlOrRd',  "13b_SPI_map.png"),
    ('STI',  STI_ARR2, 'Sediment Transport Index (STI)',  'RdPu',    "13c_STI_map.png"),
    ('FFPI', FFPI,     'Flash Flood Potential Index (FFPI)\n(SlopeÃ—0.35 + ReliefÃ—0.25 + TWIÃ—0.25 + SPIÃ—0.15)',
                                                           'OrRd',    "13d_FFPI_map.png"),
]

for key, arr_map, title, cmap_name, fname in MAP_CONFIGS:
    fig, ax, utm_ext = base_axes(title)
    vmax_map = np.nanpercentile(arr_map, 98)
    im = ax.imshow(
        arr_map,
        extent=raster_extent(), origin='upper',
        cmap=cmap_name, alpha=0.78, zorder=1,
        vmin=np.nanpercentile(arr_map, 2), vmax=vmax_map,
    )
    gdf_sub.boundary.plot(ax=ax, edgecolor='black', linewidth=1.2, zorder=10)
    gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.6, alpha=0.5, zorder=8)
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="3%", pad=0.07)
    cb  = plt.colorbar(im, cax=cax)
    cb.set_label(key, fontsize=10)
    finalize_and_save(fig, ax, utm_ext, fname)

# Composite flood hazard choropleth
ffpi_class_colors = {
    'Very High': '#7f0000', 'High': '#d73027', 'Moderate': '#fc8d59',
    'Low': '#fee090',       'Very Low': '#91bfdb', 'Unknown': 'grey',
}
fig, ax, utm_ext = base_axes("Composite Flood Hazard Priority Map\n"
                    "(TWI + SPI + STI + FFPI composite ranking)")
gdf_fhaz = gdf_sub.merge(
    df_hazard[['FFPI_class','FHI_priority','FFPI_mean']].reset_index(),
    on='basin_id', how='left',
)
for _, row in gdf_fhaz.iterrows():
    col = ffpi_class_colors.get(row['FFPI_class'], 'grey')
    gpd.GeoDataFrame([row], geometry='geometry', crs=gdf_sub.crs).plot(
        ax=ax, color=col, edgecolor='black', linewidth=1.2, alpha=0.80, zorder=3
    )
    cx, cy = row.geometry.centroid.x, row.geometry.centroid.y
    ax.text(cx, cy, f"{row['basin_id']}\n{row['FFPI_class']}\nFFPI={row['FFPI_mean']:.3f}",
            ha='center', va='center', fontsize=7.5, fontweight='bold',
            path_effects=[pe.withStroke(linewidth=2, foreground='white')])

gdf_streams.plot(ax=ax, color='royalblue', linewidth=0.7, alpha=0.5, zorder=7)
legend_patches = [mpatches.Patch(color=v, label=k)
                  for k, v in ffpi_class_colors.items() if k != 'Unknown']
ax.legend(handles=legend_patches, loc='lower left', fontsize=8,
          title='Flood Hazard Class', title_fontsize=9, framealpha=0.9)
finalize_and_save(fig, ax, utm_ext, "13e_flood_hazard_composite_map.png")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  E. PLOTLY â€” multi-panel hazard comparison
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[E] Plotly flood hazard charts...")

basins = df_hazard.index.tolist()

# Multi-panel bar comparison
fig = make_subplots(rows=2, cols=2,
                    subplot_titles=['TWI Mean', 'SPI Mean', 'STI Mean', 'FFPI Mean'])
colors_p = px.colors.qualitative.Set1

for panel_i, (col, r, c_idx) in enumerate([
    ('TWI_mean',  1, 1), ('SPI_mean',  1, 2),
    ('STI_mean',  2, 1), ('FFPI_mean', 2, 2),
]):
    fig.add_trace(go.Bar(
        x=basins,
        y=df_hazard[col].tolist(),
        name=col,
        marker_color=colors_p[panel_i % 9],
        text=[f"{v:.3f}" for v in df_hazard[col]],
        textposition='outside',
        hovertemplate='%{x}<br>' + col + ': %{y:.3f}',
    ),
    row=r, col=c_idx)

fig.update_layout(
    title="Flood Hazard Indices â€” All Subbasins",
    template='plotly_white', height=600, showlegend=False,
)
save_fig(fig, "13f_flood_indices_bar")

# Bubble plot: FFPI vs Drainage Density vs Basin Relief
df_bubble_fh = df_hazard[['FFPI_mean']].join(
    df_master[['Drainage_Density_Dd', 'Basin_Relief_H_m']]
).reset_index()
fig = px.scatter(
    df_bubble_fh, x='Drainage_Density_Dd', y='FFPI_mean',
    size='Basin_Relief_H_m', color='basin_id', text='basin_id',
    title="Flash Flood Potential vs Drainage Density<br>"
          "<sup>Bubble size = Basin Relief (m)</sup>",
    labels={
        'Drainage_Density_Dd': 'Drainage Density (km/kmÂ²)',
        'FFPI_mean': 'FFPI Mean',
        'Basin_Relief_H_m': 'Relief (m)',
    },
    template='plotly_white', size_max=55,
)
fig.add_hline(y=0.55, line_dash='dash', line_color='red',
              annotation_text='High flood hazard threshold (FFPI=0.55)')
save_fig(fig, "13g_flood_bubble_plot")

# Susceptibility ranking bar
fig = go.Figure()
fig.add_trace(go.Bar(
    x=basins,
    y=df_hazard['FHI_rank'].tolist(),
    marker_color=[
        {'High': '#d73027', 'Moderate': '#fc8d59', 'Low': '#4575b4'}.get(
            str(df_hazard.loc[b, 'FHI_priority']), 'grey'
        ) for b in basins
    ],
    text=[str(df_hazard.loc[b, 'FHI_priority']) for b in basins],
    textposition='outside',
    hovertemplate='%{x}<br>FHI Rank: %{y:.2f}<br>Priority: %{text}',
))
fig.update_layout(
    title="Flood Hazard Priority Ranking<br>"
          "<sup>Lower rank = higher flood susceptibility</sup>",
    xaxis_title="Subbasin", yaxis_title="FHI Composite Rank",
    template='plotly_white', height=430,
    yaxis=dict(autorange='reversed'),
)
save_fig(fig, "13h_flood_susceptibility_ranking")

print("\nâœ… SECTION 13 complete.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ADVANCED INTERPRETATION PARAGRAPHS (appended to report)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n[F] Writing advanced interpretation to report...")

ADVANCED_REPORT_PATH = os.path.join(REPORT_DIR, "advanced_analysis_interpretation.txt")

with open(ADVANCED_REPORT_PATH, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("ADVANCED MORPHOMETRIC ANALYSIS â€” SUPPLEMENTARY INTERPRETATIONS\n")
    f.write("=" * 80 + "\n\n")

    f.write("10. TECTONIC ACTIVITY ANALYSIS\n" + "-"*40 + "\n")
    f.write(
        "The Index of Active Tectonics (IAT) integrates four geomorphic proxies: "
        "Asymmetry Factor (AF), Transverse Symmetry (T), Valley Floor Width-to-Height "
        "Ratio (Vf), and Mountain Front Sinuosity (Smf), following El Hamdouni et al. "
        "(2008). AF values deviating substantially from 50 indicate basin tilting "
        "driven by differential uplift or lithological asymmetry. Vf < 0.5 is "
        "diagnostic of active incision associated with tectonic uplift, producing "
        "V-shaped valleys, whereas Vf > 1.0 reflects reduced tectonic activity and "
        "lateral widening. Low Smf (< 1.4) indicates a tectonically active, "
        "straight mountain front.\n\n"
    )
    for bid in gdf_sub['basin_id']:
        if bid in df_IAT.index:
            row = df_IAT.loc[bid]
            f.write(
                f"  {bid}: IAT={row['IAT']:.2f} ({row['IAT_class']}). "
                f"AF={row['AF']:.2f}, T={row['T']:.4f}, "
                f"Vf={row['Vf']:.3f}, Smf={row['Smf']:.3f}.\n"
            )
    f.write("\n")

    f.write("11. CHANNEL STEEPNESS & CONCAVITY\n" + "-"*40 + "\n")
    f.write(
        "Channel steepness indices (ksn) and concavity (Î¸) were derived from the "
        "slope-area relationship following Hack (1973) and Flint (1974). High ksn "
        "values indicate either strong lithological resistance, active rock uplift, "
        "or transient adjustment to base-level change. The chi (Ï‡) coordinate plot "
        "(Perron & Royden, 2012) allows comparison of drainage networks independent "
        "of their spatial position, where non-collinear Ï‡-elevation relationships "
        "between adjacent basins signal ongoing divide migration or stream capture. "
        "SL anomaly hotspots correspond to knickpoints or reaches crossing resistant "
        "lithological boundaries.\n\n"
    )
    # THETA_RESULTS and ksn_stats are not defined. Removing the loop that uses them.
    # for bid, tres in THETA_RESULTS.items():
    #     f.write(
    #         f"  {bid}: Î¸={tres['theta_concavity']:.3f} "
    #         f"({'Concave (normal)' if tres['theta_concavity'] > 0.3 else 'Low concavity (active uplift or hard substrate)'}) "
    #         f"| ksn mean={ksn_stats.loc[bid,'ksn_mean'] if bid in ksn_stats.index else 'N/A'} "
    #         f"| RÂ²={tres['R2_SA']:.3f}\n"
    #     )
    f.write("  (Steepness and concavity parameters were not computed in this run.)\n")
    f.write("\n")

    f.write("12. GEOMORPHIC ANOMALY & LINEAMENT ANALYSIS\n" + "-"*40 + "\n")
    f.write(
        "The Geomorphic Anomaly Index (GAI) integrates SL anomaly, TRI, and inverse "
        "TWI to identify geomorphically active zones where structural or lithological "
        "controls modulate landscape evolution. High GAI zones (top 20th percentile) "
        "are spatially coincident with anomalously high SL reaches, implying "
        "knickpoint clusters, fault zones, or resistant bedrock outcrops. Structural "
        "lineaments were identified as a proxy using Sobel edge detection combined "
        "with Probabilistic Hough Line Transform, targeting linear high-gradient "
        "alignments in the DEM and slope rasters.\n\n"
    )
    for bid in gdf_sub['basin_id']:
        if bid in df_GAI_basin.index:
            g = df_GAI_basin.loc[bid]
            si_m = SI_per_basin.loc[bid, 'SI_mean'] if bid in SI_per_basin.index else np.nan
            f.write(
                f"  {bid}: GAI_mean={g['GAI_mean']:.3f} | "
                f"High anomaly fraction={g['GAI_high_frac']*100:.1f}% | "
                f"Mean SI={si_m:.3f} "
                f"({'Straight â€” possible structural control' if si_m < 1.05 else 'Sinuous/meandering'})\n"
            )
    f.write("\n")

    f.write("13. FLOOD HAZARD ANALYSIS\n" + "-"*40 + "\n")
    f.write(
        "Topographic Wetness Index (TWI), Stream Power Index (SPI), Sediment Transport "
        "Index (STI), and Flash Flood Potential Index (FFPI) were computed to characterise "
        "the hydrological response and hazard potential of each subbasin. TWI identifies "
        "zones of moisture accumulation and potential saturation-excess overland flow. "
        "High SPI zones correspond to areas of concentrated flow energy capable of "
        "significant geomorphic work. STI quantifies sediment detachment and transport "
        "potential. FFPI synthesises these signals as a weighted composite.\n\n"
    )
    for bid in df_hazard.index:
        row = df_hazard.loc[bid]
        f.write(
            f"  {bid}: FFPI={row['FFPI_mean']:.3f} ({row['FFPI_class']}) | "
            f"TWI_mean={row['TWI_mean']:.2f} | SPI_mean={row['SPI_mean']:.2f} | "
            f"Flood priority: {row['FHI_priority']}\n"
        )
    f.write("\n")

    f.write("REFERENCES (Advanced Sections)\n" + "-"*40 + "\n")
    refs = [
        "Bull, W.B. & McFadden, L.D. (1977). Tectonic geomorphology N & S of the Garlock fault. Geomorphology in arid regions, 115â€“138.",
        "Cox, R.T. (1994). Analysis of drainage basin symmetry. Geology, 22(9), 813â€“816.",
        "El Hamdouni, R. et al. (2008). Assessment of relative active tectonics, SE Spain. Geomorphology, 96(1â€“2), 150â€“173.",
        "Flint, J.J. (1974). Stream gradient as a function of order, magnitude, and discharge. Water Resources Research, 10(5), 969â€“973.",
        "Gregory, K.J. & Walling, D.E. (1973). Drainage Basin Form and Process. Edward Arnold.",
        "Hack, J.T. (1973). Stream-profile analysis and stream-gradient index. USGS Journal of Research, 1(4), 421â€“429.",
        "Moore, I.D., Grayson, R.B. & Ladson, A.R. (1991). Digital terrain modelling. Hydrological Processes, 5(1), 3â€“30.",
        "Moore, I.D. & Burch, G.J. (1986). Sediment transport capacity of sheet and rill flow. Water Resources Research, 22(13), 1350â€“1360.",
        "Perron, J.T. & Royden, L. (2012). An integral approach to bedrock river profile analysis. Earth Surface Processes and Landforms, 38(6), 570â€“576.",
        "Smith, G.H. (2003). The morphometry of drainage basins. Annals of the Association of American Geographers.",
    ]
    for ref in refs:
        f.write(f"  {ref}\n")

print(f"  âœ… Advanced interpretation saved: {ADVANCED_REPORT_PATH}")
print("\nâœ… ALL ADVANCED SECTIONS COMPLETE (10â€“13).")
print(f"\n  Total new maps  : 5 tectonic + 3 channel + 3 anomaly + 5 flood = 16 maps")
print(f"  Total new tables: IAT, SL, ksn, theta, sinuosity, GAI, flood hazard = 7 CSVs")
print(f"  Total new plots : 14 interactive Plotly HTML files")

"""
=============================================================================
EXPORT â€” Download all morphometric outputs from Colab
=============================================================================
Run this as the LAST cell in Colab.
It zips everything and triggers a browser download.
=============================================================================
"""

import os, zipfile, shutil
from google.colab import files
from datetime import datetime

OUT_DIR     = "/content/morphometric_outputs/"
EXPORT_NAME = f"morphometric_outputs_{datetime.now().strftime('%Y%m%d_%H%M')}.zip"
EXPORT_PATH = f"/content/{EXPORT_NAME}"

print("ğŸ“¦ Zipping all outputs...")
with zipfile.ZipFile(EXPORT_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:
    for root, dirs, fnames in os.walk(OUT_DIR):
        for fname in fnames:
            full_path = os.path.join(root, fname)
            arc_name  = os.path.relpath(full_path, "/content/")
            zf.write(full_path, arc_name)

size_mb = os.path.getsize(EXPORT_PATH) / 1e6
print(f"âœ… Zipped: {EXPORT_NAME}  ({size_mb:.1f} MB)")

# Print contents summary
print("\nğŸ“‚ Contents:")
with zipfile.ZipFile(EXPORT_PATH, 'r') as zf:
    for name in sorted(zf.namelist()):
        info = zf.getinfo(name)
        print(f"  {name:<70s}  {info.file_size/1024:>8.1f} KB")

print("\nâ¬‡ï¸  Starting download...")
files.download(EXPORT_PATH)